<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[serverless]]></title>
    <url>%2F2019%2F06%2F02%2FServerless%E6%9E%B6%E6%9E%84_new%2F</url>
    <content type="text"><![CDATA[本文整理自Mike Roberts的文章，原文链接：https://martinfowler.com/articles/serverless.html，本文首先用较长的篇幅举例说明了什么是Serverless，从不同角度阐述了Serverless的利弊；并且与现有的系统架构做了对比，引入其他业界权威人士关于Serverlss的讨论，到目前为止这是我看到的对Serverless表述最全面的文章。 Serverless架构 Serverless架构是将第三方“Backend as a Service”（BaaS）服务和/或包括在“Function as a Service”（FaaS）平台上托管的、短暂的容器中运行的自定义代码的应用程序设计。通过使用这些以及相关的思想（如单页应用程序），这种体系结构消除了对传统的始终在线服务器组件的大量需求。Serverless架构可能受益于显著降低的运营成本、复杂性和工程交付周期，同时增加了对供应商依赖性和相对不成熟的支持服务的依赖。 什么是Serverless架构可以将Serverless理解为两个不同却又有重叠的两个部分： 首先，Serverless用于描述大量或完全结合第三方、云托管应用程序和服务的应用程序，以管理服务器端逻辑和状态。这些通常是“富客户端”应用程序，例如单页Web应用程序，或使用云可访问数据库（如Parse、FireBase）、身份验证服务（如Auth0、AWS Cognito）等庞大生态系统的移动应用程序。这些类型的服务以前被描述为“(Mobile) Backend as a Service”。 Serverless也可以表示服务器端逻辑仍由应用程序开发人员编写的应用程序，但与传统体系结构不同，它运行在无状态计算容器中，这些容器是事件触发的、短暂的（可能只持续一次调用），并且完全由第三方管理。考虑到这一点的一种方法是“Function as a Service”或“FaaS”。 例子UI驱动的应用（UI-driven applications）让我们考虑一个具有服务器端逻辑的传统的面向客户端的三层架构系统。一个很好的例子是一个典型的电子商务应用程序，比如说一个在线宠物商店：传统上，架构如下图所示。假设它是用服务器端是用Java或JavaScript实现的，HTML+JavaScript组件作为客户端： 这样架构相对来说不是特别智能，因为在服务器应用中实现了太多的系统逻辑，比如认证授权，页面导航，搜索，事务。 对于Serverless架构，最终可能会看起来更像这样： 这是一个大大简化的视图，但即使在这里，我们也看到了一些重要的变化： 我们删除了原始应用程序中的身份验证逻辑，并将其替换为第三方BaaS服务（例如，auth0）。 使用BaaS的另一个例子，我们允许客户直接访问我们的数据库的一个子集（用于产品列表），它本身完全由第三方（如Google FireBase）托管。与访问数据库的服务器资源相比，我们可能有不同的客户访问数据库的安全配置文件。 前两点暗示了一个非常重要的第三点：宠物店服务器中的一些逻实现在客户端中——例如，跟踪用户会话、了解应用程序的UX结构、从数据库中读取数据并将其转换为可用视图等。客户机正在很好地成为单页应用程序（Single Page Application）。 我们可能希望在服务器中保留一些与UX相关的功能，例如，如果它是计算密集型的，或者需要访问大量数据。在我们的宠物店中，一个例子是“搜索”。我们可以实现FaaS功能，通过API网关响应HTTP请求（稍后描述），而不是像原始体系结构中那样始终运行的服务器。客户机和服务器的“搜索”功能都从同一数据库中读取产品数据。 最后，我们可以用另一个单独的FaaS Function替换我们的“购买”功能，出于安全原因选择将其保留在服务器端，而不是在客户端中重新实现。它前面也有一个API网关。在使用FaaS时，将不同的逻辑需求分解为单独部署的组件是一种非常常见的方法。 退一步说，这个例子演示了关于Serverless架构的另一个非常重要的点。在原始版本中，所有流、控制和安全都由中央服务器应用程序管理。在Serverless版本中，这些问题没有中央仲裁器。相反，我们看到了一个偏好—-choreography over orchestration，每个组件都扮演着一个更具体系结构意识的角色，这也是微服务方法中常见的一个想法。 这种方法有很多好处。正如Sam Newman在他的《构建微服务》一书中指出的那样，以这种方式构建的系统通常“更灵活，更易于更改”，无论是作为一个整体，还是通过对组件的独立更新；对问题进行了更好的拆分；还有成本上的好处，这一点Gojko Adzic在这篇精彩的演讲中讨论过。 当然，这样的设计是一种权衡：它需要更好的分布式监控，而且我们更依赖于底层平台的安全功能。从根本上讲，我们需要处理的移动部件比我们最初使用的单片应用程序要多得多。灵活性和成本的好处是否值得增加多个后端组件的复杂性，这取决于上下文。 消息驱动的应用（Message-driven applications）另一个例子是后端数据处理服务。 假设您正在编写一个以用户为中心的应用程序，它需要快速响应UI请求，其次，它需要捕获正在发生的所有不同类型的用户活动，以便进行后续处理。想想一个在线广告系统：当用户点击一个广告时，你想很快地将他们重定向到该广告的目标。同时，你需要收集点击已经发生的事实，这样你就可以向广告商收费。 传统上，架构可能如下所示。“Ad Server”同步响应用户（图中未显示），并向Channel发送“点击消息”。然后，“Click Processor”应用程序将异步处理此消息，更新数据库，例如减少广告客户的预算。 在Serverless的世界中，这看起来如下： 你能看到区别吗？与第一个例子（Pet store）相比，这里的架构变化要小得多，这就是为什么异步消息处理是Serverless技术非常流行的用例。我们已经用FaaS Function替换了一个长期使用的消息消费者应用程序。此函数在供应商提供的事件驱动上下文中运行。请注意，云平台供应商同时提供消息代理和FaaS环境，这两个系统彼此紧密相连。 FaaS环境还可以通过实例化函数代码的多个副本并行处理多个消息。这取决于我们如何编写原始过程（original process），这可能是我们需要考虑的一个新概念。 深入Function as a Service（Unpacking “Function as a Service”）我们已经提到了很多关于FaaS的东西，是时候深入去看它到底意味着什么。为此，我们首先来看一下亚马逊FaaS产品Lambda的描述。作者在上面加了一些注释： AWS Lambda lets you run code without provisioning or managing servers. (1) … With Lambda, you can run code for virtually any type of application or backend service (2) - all with zero administration. Just upload your code and Lambda takes care of everything required to run (3) and scale (4) your code with high availability. You can set up your code to automatically trigger from other AWS services (5) or call it directly from any web or mobile app (6)*. * 从根本上讲，FaaS是在不管理你的服务器系统或你的”长期运行的服务器应用程序“的情况下运行后端代码。当与容器和PaaS（平台即服务）等现代体系结构趋势进行比较时，第二条“长期运行的服务器应用程序”是一个关键区别。 如果我们回到前面的Click-processing的例子（就是广告那个例子），FaaS通过某种不需要服务器或者不需要应用程序一直运行的方式替换了点击处理服务器（可能是物理机器，但肯定是特定的应用程序）。 FaaS产品不需要编码到特定的框架或库中。FaaS函数在语言和环境方面是常规应用程序。例如，AWS lambda函数可以在Javascript、Python、Go、任何JVM语言（Java、Culjule、Scala等）或任何.NET语言中实现“first class”。然而，lambda函数也可以执行与另一个部署项目捆绑在一起的进程，因此您实际上可以使用任何可以编译为Unix进程的语言（参见本文后面的apex）。 尽管如此，FaaS功能具有显著的体系结构限制，尤其是在状态和执行持续时间方面。 让我们再次考虑一下我们的点击处理示例。当迁移到FaaS时，唯一需要更改的代码是“主方法”（启动）代码，因为它被删除，并且可能是顶级消息处理程序（消息侦听器接口）实现的特定代码，但这可能只是方法签名的更改。其余的代码（例如，写入数据库的代码）在FaaS世界中没有什么不同。 部署与传统系统非常不同，因为我们没有自己运行的服务器。在FaaS环境中，我们将函数的代码上传到FaaS服务提供者，从而执行配置资源、实例化虚拟机、管理进程等所需的所有其他操作。 水平伸缩是完全自动的、弹性的，并由提供者管理。如果您的系统需要并行处理100个请求，那么提供者将处理这些请求，而不需要您进行任何额外的配置。执行函数的“计算容器”是短暂的，FaaS提供者创建和销毁它们纯粹是由运行时需求驱动的。最重要的是，有了FaaS，供应商可以处理所有底层的资源配置和分配，用户根本不需要集群或虚拟机管理。 让我们回到Click-processor。假设我们今天过得很愉快，客户点击的广告数量是平常的十倍。对于传统的体系结构，我们的点击处理应用程序能够处理这个问题吗？例如，我们是否开发了能够一次处理多条消息的应用程序？如果我们这样做了，一个正在运行的应用程序实例是否足以处理负载？如果我们能够运行多个进程，缩放是自动的还是需要手动重新配置？使用FaaS的方式，所有这些问题都已经得到了解答，您需要提前编写函数来假设水平伸缩的并行性，但是从那时起FaaS提供程序会自动处理所有缩放需求。 FaaS中的函数通常由提供程序定义的事件类型触发。对于Amazon AWS，此类刺激包括S3（文件/对象）更新、时间（计划任务）和添加到消息总线的消息（例如，Kinesis）。 大多数提供者还允许函数作为对入站HTTP请求的响应而触发；在AWS中，通常通过使用API网关来实现这一点。我们在宠物店中使用了API网关作为“搜索”和“购买”功能的示例。函数也可以通过平台提供的API直接调用，可以从外部调用，也可以从同一个云环境中调用，但这是一种比较少见的用法。 状态（State）当FaaS函数涉及到本地（机器/实例绑定）状态时，它有很大的限制，即存储在内存变量中的数据，或写入本地磁盘的数据。您确实有这样的存储可用，但是您不能保证这种状态在多个调用中是持久存在的，而且更强烈地说，您不应该假定一个函数的一次调用的状态对同一个函数的另一次调用是可用的。因此，FaaS函数通常被描述为无状态的，但更准确地说，FaaS函数的任何状态（需要持久化）都需要在FaaS函数实例之外进行外部化。 FaaS Function天生就是无状态的。比如，那些为输入到输出提供纯函数转换的函数，这是不需要考虑状态的。但对于其他人来说，这（无状态）可能会对应用程序体系结构产生很大的影响，尽管这不是一个独特的限制条件，“Twelve-Factor app”概念有着完全相同的限制。这种面向状态的函数通常使用数据库、跨应用程序缓存（如Redis）或网络文件/对象存储（如S3）来存储跨请求的状态，或提供处理请求所需的进一步输入。 执行时间（Execution duration）FaaS函数通常允许限制每次调用的时间。目前，对一个AWS lambda函数响应事件的“超时”最多是在被终止之前的五分钟。Microsoft Azure和Google云功能也有类似的限制。 这意味着，如果没有重新架构（re-architecture），某些长时间运行的任务类不适合FaaS函数，您可能需要创建几个不同的相互协作的FaaS函数，而在传统环境中，您可能有一个长时间运行任务正在执行和相互协作。 启动延迟和”冷启动”(Startup latency and “cold starts”)FaaS平台在每个事件之前初始化函数的实例需要一些时间。即使对于一个特定的函数，启动延迟也会有很大的变化，这取决于大量的因素，并且可能在几毫秒到几秒之间。这听起来很糟糕，但让我们更具体一点，以AWS lambda为例。 lambda函数的初始化要么是“热启动”——从以前的事件中重用lambda函数及其主机容器的实例，要么是“冷启动”——创建一个新的容器实例，启动函数主机进程等。毫无疑问，在考虑启动延迟时，冷启动最值得关注。 冷启动延迟取决于许多变量：您使用的语言，您使用的库数量，您拥有的代码数量，Lambda函数环境本身的配置，是否需要连接到VPC资源等等。这些方面受开发人员的控制，因此通常可以减少作为冷启动的一部分而产生的启动延迟。 和冷启动时间同样的因素是冷启动的频率。例如，如果一个函数每秒处理10个事件，每个事件需要50毫秒来处理，那么每隔100,000-200,000个事件，您可能只会看到Lambda的冷启动。另一方面，如果您每小时处理一次事件，您可能会看到每个事件的冷启动，因为Amazon会在几分钟后退出非活动的Lambda实例。了解这一点有助于您了解冷启动是否会对聚合产生影响，以及您是否希望对您的函数实例执行“保持活跃”，以避免它们被放在pasture上。 冷启动需要被关注吗？这取决于您的应用程序的风格和流量类型。作者在Intent Media的前团队有一个用Java实现的异步消息处理Lambda应用程序（通常是启动时间最慢的语言），每天处理数亿条消息，他们不关心这个组件的启动延迟。也就是说，如果您正在编写一个低延迟交易应用程序，那么您可能不希望此时使用云托管的FaaS系统，无论您使用何种语言进行实施都没有关系。 无论您是否认为您的应用程序可能存在这样的问题，您都应该使用类似产品的负载测试性能。 有关冷启动的更多详细信息，请参阅。 API Gateway 前面提到的Serverless的一个方面是“API网关”。API网关是一个HTTP服务器，其中路由（Routes）和端点（endpoints）是在配置中定义的，并且每个路由都与处理该路由的资源相关联。在Serverless架构中，此类处理程序通常是FaaS函数。 当API网关收到请求时，它会找到与请求匹配的路由配置，如果有一个路由和一个FaaS函数匹配，原始请求将会调用相关的FaaS函数。通常，API网关将允许从HTTP请求参数映射到FaaS函数更简洁的输入，通常为JSON对象，或者允许整个HTTP请求通过。FaaS函数将执行其逻辑并将结果返回到api网关，然后将此结果转换为HTTP响应，并将其传递回原始调用方。 AWS有自己的API网关（有点让人困惑的名字叫“API网关”），其他供应商也提供类似的功能。亚马逊的API网关是一个BaaS（是的，BaaS！）服务本身就是一个服务，因为它是一个您配置的外部服务，但不需要自己运行或提供。 除了纯粹的路由请求之外，API网关还可以执行身份验证、输入验证、响应代码映射等。 有FaaS函数的api网关的一个用例是以Serverless方式创建HTTP前端的微服务，它具有FaaS功能带来的所有扩展、管理和其他好处。 What isn’t Serverless?到目前为止，在本文中，我已经将Serverless描述为两种思想的结合：BaaS和FaaS。我还深入研究了后者的能力。为了更准确地了解我所看到的Serverless服务的关键属性（以及为什么我认为更老的服务（如S3）是Serverless的），参考：定义Serverless。 在我们开始研究非常重要的优点和缺点之前，我想在定义上再花一点时间。让我们定义一下什么不是Serverless。 和PaaS对比考虑到Serverless FaaS 函数与十二要素应用程序非常相似，它们是否只是“Platform as a Service”（paas）的另一种形式，如heroku？简单地说，我引用了Adrian Cockcroft的推特： 换句话说，大多数PaaS应用程序并不是为了响应事件而将整个应用程序启动和关闭，而FaaS平台恰恰是这样做的。 如果我是一个优秀的十二要素应用程序开发人员，这不一定影响我如何编程和设计我的应用程序，但它确实对我如何运维有很大的影响。因为我们都是很好DevOps-savvy工程师，所以我们对运维的思考和对开发的思考一样多，对吧？ FaaS和PaaS之间的关键操作区别在于可伸缩性。一般来说，对于PaaS，您仍然需要考虑如何缩放，例如，对于Heroku，您希望运行多少个Dyno？对于FaaS应用程序，这是完全透明的。即使将您的PaaS应用程序设置为自动缩放，您也不会将此设置为单个请求的级别（除非您有一个非常具体的流量配置文件），因此在成本方面，FaaS应用程序更高效。 考虑到这个好处，你为什么还要使用PaaS？有几个原因，但工具可能是最大的。还有一些人使用像CloudFoundry这样的PaaS平台，在公共云和私有云混合的基础上提供共同的开发体验；在撰写本文时，还没有一个与此相当成熟的FaaS。 和容器对比使用无服务器FaaS的原因之一是避免在操作系统级别管理应用程序进程。PaaS服务，如Heroku，也提供了这种功能，我在上面已经描述了PaaS与Serverless FaaS的区别。另一个流行的进程抽象是容器，Docker是这种技术最明显的例子。容器托管系统（如Meos和Kubernetes）从操作系统级部署中抽象出单个应用程序，越来越受欢迎。更进一步，我们可以看到云托管容器平台（如Amazon ECS和EKS）和谷歌容器引擎（如无服务器FaaS），它们让团队完全不必管理自己的服务器主机。考虑到容器的势头，考虑无服务器FaaS是否仍然值得？ 主要地，我对PaaS的观点仍然适用于容器——对于Serverless的FaaS，伸缩是自动管理、透明和细粒度的，这与我前面提到的自动资源配置和分配有关。传统上，容器平台仍然需要您管理集群的大小和形状。 我还认为容器技术还不成熟和稳定，尽管它越来越接近成熟和稳定（作者的文章写于2017年，当时的容器技术还不够成熟）。当然，这并不是说无服务器FaaS已经成熟了，从两者中挑选一个依然是一个需要考虑的事。 同样重要的是，现在可以在容器平台中使用自扩展容器集群。 Kubernetes内置了“Horizontal Pod Autoscaling”，而像AWS Fargate这样的服务也实现了“Serverless Container”的承诺。 当我们看到无服务器FaaS和托管容器之间的管理和扩展差距缩小时，它们之间的选择可能只取决于应用程序的样式和类型。例如，FaaS可能被视为事件驱动样式的更好选择，每个应用程序组件的事件类型很少，容器被视为具有许多入口点（Entry Point）的同步请求驱动（synchronous-request–driven）组件的更好选择。我希望在很短的时间内，许多应用程序和团队都会使用这两种体系结构方法，并且看到这种使用模式的出现将会非常有吸引力。 No OpsServerless并不意味着“No Ops” - 虽然它可能意味着“没有系统管理员”（No sysadmin），这取决于你去的Serverless兔子洞的距离（depending on how far down the Serverless rabbit hole you go）。 “Ops”比”server administration”意味着更多。它还至少意味着：监控，部署，安全性，网络，支持，以及通常一些生产调试和系统扩展。这些问题在Serverless应用程序中仍然存在，您仍然需要一个策略来处理它们。在某些方面，Ops在Serverless世界中更难，因为其中很多都是如此新潮。 Sysadmin依然存在，只是你外包给了Serverless。这并不是一件坏事或者好事——我们有很多外包，它的好处和坏处依赖于具体你尝试要去做的事情。无论哪种方式，在某些时候抽象可能会泄漏，你需要知道某个地方的人类系统管理员正在支持你的应用程序。 Charity Majors在第一个Serverlessconf上就这个主题进行了很好的讨论。 （您还可以阅读她的两篇文章：WTF is operations?和Operational Best Practices。） 存储过程即服务（Stored Procedures as a Service）我看到的另一个主题是Serverless FaaS就是“存储过程即服务”（Stored Procedures as a Service）。我认为这是因为很多FaaS函数的例子（包括我在本文中使用的一些）都是一小段代码，并且与数据库紧密集成。如果这就是我们可以使用FaaS的全部，我认为这个名称会很有用，但因为它实际上只是FaaS功能的一个子集，所以我认为用这些术语来思考FaaS是没有用的。 话虽如此，值得考虑FaaS是否存在一些存储过程相同的问题，包括Camille在上述推文中提及的技术债务问题。使用存储过程可以获得许多教训，这些存储过程值得在FaaS环境中进行检查并查看它们是否适用。 考虑一下存储过程： 通常需要特定于供应商的语言，或者至少需要特定于供应商的语言框架/扩展。 很难测试，因为它们需要在数据库的上下文中执行。 对版本控制或作为一等应用程序（first class application）处理是棘手的。 虽然并非所有这些都必然适用于存储过程的所有实现，但它们肯定会遇到问题。让我们看看它们是否适用于FaaS： For（1）对于我到目前为止看到的FaaS实现绝对不是一个问题，因此我们可以立即从列表中删除它。 For（2）因为我们正在处理“只是代码”，所以单元测试绝对和其他任何代码一样简单。集成测试虽然是一个不同的问题，我们将在后面讨论。 For（3）再次，因为FaaS功能是“只是代码”版本控制是可以的。直到最近，应用程序打包也是一个问题，但是已经逐渐成熟，使用亚马逊的Serverless Application Model（SAM）和前面提到的Serverless框架等工具。在2018年初，亚马逊甚至推出了“Serverless Application Repository”（SAR），为组织提供了一种基于AWS Serverless服务构建应用程序和应用程序组件的方法。 （更多关于SAR参考：“Examining the AWS Serverless Application Repository”） Benifits到目前为止，我主要解释了Serverless Architecture的含义。现在，我将讨论这种设计和部署应用程序的方法的一些优点和缺点。如果没有反复思考和权衡利弊，你绝对不应该决定使用Serverless。 让我们从rainbows and unicorns的地方开始，看看Serverless的好处。 降低运维成本(Reduced operational cost) Serverless是最简单的外包解决方案。它允许您向某人付费以管理您自己可能管理的服务器，数据库甚至应用程序逻辑。由于您正在使用许多其他人也将使用的预定义服务，因此我们看到了规模经济（Economy of Scale）效应：因为一个供应商运行着数千个非常相似的数据库，所以你为管理数据库的花费就越低。 (可以简单理解为：因为量大，所以便宜)。 降低的成本在两个方面表现为总和。首先是纯粹来自与其他人共享基础设施（例如，硬件，网络）的基础设施成本增益。第二个是人工成本增加：相比自己开发和管理，您可以在外包的Serverless系统上花费更少的时间。 但是，这种好处与您从基础架构即服务（IaaS）或平台即服务（PaaS）中获得的好处并没有太大差别。但我们可以通过两种主要方式扩展这种优势，每种方式都适用于无服务器BaaS和FaaS。 BaaS：降低了开发成本(BaaS: reduced development cost)IaaS和PaaS基于服务器和操作系统管理可以商品化的前提。另一方面，Serverless BaaS是整个应用程序组件商品化的结果。 身份验证是一个很好的例子许多应用程序编写自己的身份验证功能，这些功能通常包括注册，登录，密码管理以及与其他身份验证提供程序集成等功能。总的来说，这种逻辑在大多数应用程序中非常相似，并且已经创建了像Auth0这样的服务，以允许我们将现成的身份验证功能集成到我们的应用程序中，而无需我们自己开发它。 同样对于BaaS databases，例如Firebase’s database service。一些移动应用程序团队发现让客户端直接与服务器端数据库通信是有意义的。 BaaS数据库消除了大部分数据库管理开销，并且通常提供以Serverless应用程序所期望的模式对不同类型的用户执行适当授权的机制。 根据您的背景，这些想法可能会让您感到不安（可能出于我们将在缺陷部分介绍的原因）。但是不可否认的是，那些成功的公司，他们的引人注目的产品可以没有任何服务端代码。Joe Emison在第一届Serverless Conference上给了几个例子。 FaaS: 伸缩成本(FaaS: scaling costs)正如前文提到的那样，Serverless FaaS最大的好处就是完全自动化的、弹性的和被第三方托管的横向伸缩。 这有几个好处，但在基本的基础架构方面，最大的好处是您只需支付所需的计算，在AWS Lambda的情况下，最低可达100ms。依赖于你的流量规模和类型，这将带来巨大的经济收益。 示例：间隔性的请求(Example: occasional requests)假设您正在运行的服务器应用程序每分钟只处理一个请求，处理每个请求需要50毫秒，而您在一小时内的平均CPU使用率为0.1％。如果将此应用程序部署到其自己的专用主机，那么这非常低效。其他一千个类似的应用程序都可以共享这台机器。 无服务器FaaS捕获了这种低效率，以降低的成本带来收益。使用上面的示例应用程序，您每分钟只需支付100毫秒的计算费用，这是整个时间的0.15％。 这具有以下连锁效益： 对于具有非常小的负载要求的潜在微服务，它支持按逻辑/域分解组件，即使这种精细粒度的操作成本可能已经过高。 这样的成本效益是一个伟大的民主化者。如果公司或团队想要尝试一些新的东西，那么当他们使用FaaS满足他们的计算需求时，可以在很低的成本下开始一段新的旅程（dipping their toe in the water）。实际上，如果您的总工作量相对较小（但并非完全无关紧要），由于某些FaaS供应商提供的“免费套餐”（free tier），您可能根本不需要为任何计算付费。 示例：变化的流量让我们来看另外一个例子。假如你的流量是起伏不定的，并且你的流量的baseline是每秒处理20个请求，但是每间隔5分钟，你每秒会收到20个请求，持续10秒。为了示例，我们还假设您的基准性能（baseline performance）最大化了您的首选主机服务器类型，并且您不希望在流量峰值阶段减少响应时间。你怎么解决这个问题？ 在传统环境中，您可能需要将总硬件数量增加10倍，而不只是处理峰值的情况，即使峰值的总持续时间不到总机器正常运行时间的4％。自动扩展可能不是一个好的选择，因为新的实例启动时，服务器需要长时间才能启动，此时峰值阶段将结束。 对于Serverless FaaS来说这并不是一个问题。你的流量配置不会发生任何改变，只需要为波峰阶段的流量额外的计算付费就行。 显然，我故意在这里选择Serverless FaaS可以节省大量成本的示例，但重点是从扩展的角度来看，FaaS可以为你节约成本。除非你有一个一直使用服务器主机的整个容量、非常稳定的流量形状。 关于上述内容的一个警告：如果您的流量是统一的并且能够始终如一地利用正在运行的服务器，您可能看不到这种成本效益，并且您实际上使用FaaS可能花费更多。您应该进行一些数学运算，并将当前的提供商成本开销与运行全时服务器进行比较，以确定成本是否可接受。 有关FaaS成本效益的更多详细信息，我建议使用该文章Serverless Computing: Economic and Architectural Impact。 优化是节约成本的根本(Optimization is the root of some cost savings)对于FaaS成本还有一个有趣的方面：您对代码进行的任何性能优化不仅会提高应用程序的速度，而且还会直接、立即导致运维成本的降低，这具体取决于您的供应商的收费方案的粒度。 例如，假设应用程序最初需要一秒钟来处理事件。 如果通过代码优化将其减少到200毫秒，它将（在AWS Lambda上）立即看到计算成本节省80％而不进行任何基础架构更改。 运维管理变得更加简单(Easier operational management)下一部分附带一个巨大的星号 - 运维的某些方面对于Serverless来说仍然很难，但是现在我们仍然坚持我们的rainbows and unicorns朋友。 在Serverless的BaaS方面，很明显为什么运维管理比其他架构更简单：支持更少的组件等于更少的工作。 在FaaS方面，有许多方面在起作用，我将深入研究其中的几个方面。 FaaS的伸缩收益不仅仅是基础设施的成本方面（Scaling benefits of FaaS beyond infrastructure costs）经过上一节的描述，伸缩(Scaling)在我们脑海中依然是一个新东西。值得注意的是，FaaS的函数级的扩展不仅降低了计算成本，还简化了运维管理，因为伸缩是自动的。 在最好的情况下，如果您的伸缩过程是手动的，那么需要人明确地向一组服务器添加和删除实例 - 使用FaaS，您可以高兴地忘记这一点并让您的FaaS供应商为您伸缩您的应用程序。 即使您已经在非FaaS架构中使用自动伸缩，但这仍然需要设置和维护。 FaaS不再需要这项工作。 类似地，因为扩展是提供商基于每个请求或事件的 ，因此您不再需要考虑在内存不足或看到太多性能冲击之前可以处理多少并发请求的问题 - 至少系统中的托管的FaaS组件不用考虑这个问题。 考虑到其负载可能显着增加，必须重新考虑下游数据库和非FaaS组件。 降低了打包和部署的复杂度（Reduced packaging and deployment complexity）与部署整个服务器相比，打包和部署FaaS Function非常简单。您所做的就是将所有代码打包到一个zip文件中，然后上传它。没有Puppet / Chef，没有启动/停止shell脚本，也没有关于是否在计算机上部署一个或多个容器的决定。如果您刚开始使用，甚至不需要打包任何东西 - 您可以在供应商控制台本身编写代码（显然，这不建议用于生产环境的代码！）。 这个过程不需要花费很长时间来描述，但对于一些团队而言，这种好处可能非常巨大：完全的Serverless解决方案需要零系统管理。 PaaS解决方案具有类似的部署优势，但正如我们之前所看到的，在将PaaS与FaaS进行比较时，扩展优势是FaaS独有的。 上市时间和持续实验（Time to market and continuous experimentation）更容易的运维管理是我们工程师所理解的好处，但这对我们的业务意味着什么？ 显而易见的原因是成本：花在操作上的时间减少等于操作所需的人数减少，正如我已经描述的那样。但在我看来，一个更重要的原因是上市时间。随着我们的团队和产品越来越多地面向精益和敏捷流程，我们希望不断尝试新事物并快速更新现有系统。虽然在持续交付的情况下进行简单的重新部署可以快速迭代稳定的项目，但是具有良好的新想法到初始部署（new-idea-to-initial-deployment）能力使我们能够以低摩擦和最低成本尝试新的实验。 FaaS的新想法到初始部署故事通常非常出色，特别是对于由供应商生态系统中的成熟定义事件触发的简单功能。例如，假设您的组织已经在使用AWS Kinesis（一种类似Kafka的消息传递系统），通过您的基础架构广播各种类型的实时事件。使用AWS Lambda，您可以在几分钟内针对该Kinesis流开发和部署新的生产事件监听器 - 您可以在一天内尝试几个不同的实验！ 虽然成本效益是Serverless最容易表达的改进，但这种缩短的交付时间让我最兴奋。它可以实现持续实验的产品开发思维，这是我们如何在公司中交付软件的真正革命。 “更绿色的”计算？(“Greener” computing?)在过去的几十年中，世界上数据中心的数量和规模都在大幅增加。除了建立这些中心所需的物质资源外，相关的能源需求如此之大，以至于苹果，谷歌等都在谈论将一些数据中心托管在可再生能源附近以减少化石燃料燃烧的影响本来是必要的。 空闲的，但是通电的，服务器消耗了大量的能源 - 而且它们是我们需要这么多，更大的数据中心的重要原因之一： 商业和企业数据中心中的典型服务器在一年中平均提供其最大计算输出的5％到15％。 — Forbes 这非常低效，并产生巨大的环境影响。 一方面，云基础设施可能已经帮助减少了这种影响，因为公司可以按需“购买”更多的服务器，只有当他们绝对需要时，而不是提前很长时间提供所有可能必需的服务器。 然而，人们还可以争辩说，如果没有足够的容量管理，很多服务器都会被丢弃，那么provision服务器的容易程度可能会使情况变得更糟。 无论我们使用自托管服务器，IaaS还是PaaS基础架构解决方案，我们仍然会手动制定关于我们的应用程序的容量决策，这些决策通常会持续数月或数年。通常，我们对管理容量持谨慎态度，因此我们过度提供资源（over-provison），导致刚才描述的效率低下。使用Serverless方法，我们不再自己做出这样的容量决策 - 我们让Serverless供应商为我们的实时需求提供足够的计算容量。然后，供应商可以在其客户中汇总做出自己的容量决策。 与传统的容量管理方法相比，这种差异应该可以更有效地跨数据中心使用资源，从而减少对环境的影响。 缺陷(Drawbacks)所以，亲爱的读者，我希望你在彩虹，独角兽以及所有闪亮而美好的事物中享受你的时光，因为我们即将被现实中的湿鱼给予一记耳光。 有很多原因让我们喜欢Serverless架构，但是使用Serverless架构前依然需要权衡利弊。有些利弊是这个概念所固有的（trade-offs are inherent to the concepts），并不能通过持续的优化被完全解决掉，所以这些利弊总是需要被考虑。其他的问题已经和现有的实现联系到一起，随着时间流逝，这些问题，总是会被解决掉的。 固有的缺陷（Inherent drawbacks）供应商控制（Vendor control）通过任何外包策略，您可以将某些系统的控制权交给第三方供应商。这种缺乏控制可能表现为系统停机，意外限制，成本变化，功能丧失，强制API升级等。我之前提到的Charity Majors在这篇文章的权衡部分更详细地解释了这个问题： [供应商服务]，如果它是智能的，将对您如何使用它施加强大的限制，因此他们更有可能实现其可靠性目标。当用户具有灵活性和选项时，会产生混乱和不可靠性。如果平台必须在您的幸福与成千上万的其他客户的幸福之间做出选择，那么他们每次都会选择多人 - 就像他们应该的那样。 – Charity Majors 多租户问题（Multitenancy problems）多租户是指多个不同客户（或租户）的多个软件实例在同一台机器上运行，并且可能在同一主机应用程序中运行的情况。这是我们前面提到的实现规模经济效益的战略。服务供应商尽最大努力让客户觉得他们每个人都是唯一使用他们系统的人，通常优秀的服务供应商也能做得很好。但是，没有完美的方案，有时多租户解决方案可能存在安全问题（一个客户能够看到另一个客户的数据），稳健性（一个客户软件中的错误导致另一个客户的软件出现故障）和性能（高负载客户）导致另一个人放慢速度）。 这些问题并非Serverless系统所独有 - 它们存在于使用多租户的许多其他服务产品中。 AWS Lambda现在已经足够成熟，我们不希望看到这些问题，但是您应该关注任何不太成熟的服务的问题，无论是来自AWS还是其他供应商。 供应商绑定（Vendor lock-in）您从一个供应商处使用的Serverless功能很可能会被另一个供应商以不同方式实现。如果您想切换供应商，您几乎肯定需要更新您的操作工具（部署，监控等），您可能需要更改您的代码（例如，以满足不同的FaaS界面），您甚至可能如果竞争厂商的实施行为存在差异，则需要更改您的设计或架构。 即使您设法轻松迁移生态系统的一部分，您也可能受到另一个体系结构组件的更大影响。例如，假设您使用AWS Lambda来响应AWS Kinesis消息总线上的事件。 AWS Lambda，Google Cloud Functions和Microsoft Azure Functions之间的差异可能相对较小，但您仍然无法将后两个供应商实施直接连接到您的AWS Kinesis流。这意味着，如果不移动基础架构的其他模块，则无法将代码从一个解决方案移动或移植到另一个解决方案。 很多人都对这个想法感到害怕 - 要知道如果你今天所选择的云供应商明天需要改变，意味着你还有很多工作要做，那就不是很好了。因此，有些人采用“多云”（Muiti-cloud）方法，以与所使用的实际云供应商无关的方式开发和运营应用程序。这通常比单云方法更昂贵 - 因此虽然供应商绑定（Vendor lock-in）是一个合理的问题，但我仍然建议选择一个您满意的供应商并尽可能地利用他们的能力。我在这篇文章中更多地讨论了为什么会这样。 安全问题（Security concerns）采用Serverless方法可以解决大量安全问题。这里只是一些细微的事情要考虑 - 一定要探索还有什么可以影响你。 您使用的每个Serverless供应商都会增加您的生态系统所包含的不同安全设施的数量。这增加了恶意攻击的风险，并增加攻击成功的可能性。 如果直接从您的移动平台使用BaaS数据库，您将失去一个服务器端应用程序在传统应用程序中提供的保护屏障。虽然这不是一个dealbreaker，但在设计和开发应用程序时需要非常小心。 当您的组织接受FaaS时，您可能会遇到整个公司内部FaaS函数的寒武纪大爆发。这些函数中的每一个都提供了另一个问题向量。例如，在AWS Lambda中，每个Lambda函数通常与配置的IAM policy齐头并进，这很容易出错。这不是一个简单的主题，也不是一个可以忽略的主题。 IAM管理需要仔细考虑，至少在生产AWS账户中是这样。 跨客户端平台的逻辑重复（Repetition of logic across client platforms）使用“完整”的BaaS架构，服务器端不会编写自定义逻辑 - 它都在客户端中。这对您的第一个客户端平台来说可能没问题，但是只要您需要下一个平台，您就需要重复执行该逻辑的一个子集 - 您不需要在更传统的架构中重复干这些事情。例如，如果在这种系统中使用BaaS数据库，那么您的所有客户端应用程序（可能是Web，native iOS和native Android）现在都需要能够与您的供应商数据库进行通信，并且需要了解如何映射数据库schema到应用程序逻辑。 此外，如果你想在任何时候迁移到新的数据库，你需要在所有客户端复制并且协调代码。 失去了服务器的优化（Loss of server optimizations）使用完整的BaaS架构，您无法为客户端性能来优化服务器设计以。 “Backend For Frontend”模式的存在是为了在服务器中抽象整个系统的某些底层方面，部分原因是客户端可以更快地执行操作，并且在移动应用程序中使用更少的电池电量。这种模式不适用于完整的BaaS。 这一点和前一点都存在于完整的BaaS体系结构中，其中所有自定义逻辑都在客户机中，只有后端服务是由供应商提供的。这两种方法的一个缓解措施是采用FaaS，或者其他类型的轻量级服务器端模式，将某些逻辑移动到服务器上。 Serverless FaaS没有服务器内状态（No in-server state for Serverless FaaS）在经历了几个特定于BaaS的缺点之后，我们暂时谈谈FaaS。我刚才说过： 当涉及到本地状态时，FaaS功能有很大的限制。 ..您不应该假设一次调用函数的状态可用于同一函数的另一次调用。 这种假设的原因是，使用FaaS，我们通常无法控制函数的主机容器何时启动和停止。 我之前也说过，本地状态的替代方案是遵循Twelve-Factor应用程序的第六个factor，即： Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service, typically a database. – The Twelve-Factor App Heroku推荐这种思维方式，但是你可以在运行他们的PaaS时违反这个规则，因为你可以控制Heroku Dynos的启动和停止时间。使用FaaS则不会违反这个规则。 既然你不能将状态存储在内存中，那么如何在FaaS中使用状态呢？上面的引用是使用数据库，但在更多情况下是使用更快的NoSQL数据库，进程外缓存（比如：Redis），或者外部对象/文件存储（比如：S3）都会是你的选择。但是这些方法都会比内存或本机存储要慢一些。你需要考虑你的应用程序是否适合这些情况。 在这方面的另一个问题是内存缓存。许多应用从外部读取大型数据集并将其子集存储到内存中缓存中。您可能正在读取数据库中的“参考数据”表并使用Ehcache之类的东西。或者，您可能正在从指定缓存头的HTTP服务中读取，在这种情况下，内存中的HTTP客户端可以提供本地缓存。 FaaS确实允许使用本地缓存，假设您的功能足够频繁使用，这可能很有用。例如，对于AWS Lambda，我们通常期望一个函数实例可以保持几个小时，只要它每隔几分钟至少使用一次。这意味着我们可以使用Lambda可以为我们提供的（可配置的）3 GB RAM或512 MB本地“/ tmp”空间。对于某些缓存，这可能就足够了。否则，您将不再需要进程内缓存，并且您需要使用像Redis或Memcached这样的低延迟外部缓存。但是，这需要额外的工作，并且根据您的使用情况可能会非常慢。 实施缺陷（Implementation drawbacks）以前描述的缺点可能总是存在于Serverless架构中。我们会看到缓和解决方案的改进，但它们总会在那里。 然而，其余的缺点完全取决于现有技术水平。随着供应商和/或英雄的社区的意愿和投入，这些问题都可以被消灭。事实上，自从本文的第一个版本以来，这个drawback列表已经缩小了。 配置（Configuration）当我编写本文的第一个版本时，AWS几乎没有提供Lambda函数的配置方式。我很高兴地说现在已经修复了，但是如果你使用一个不太成熟的平台，它仍然是值得检查的东西。 对自己进行DoS攻击（Dos yourself）这里有一个例子，说明为什么当你处理FaaS时，”买者自负”（caveat emptor）是一个关键词。AWS Lambda限制您在给定时间可以运行的Lambda函数的并发执行次数。说这个限制是一千;这意味着您可以随时执行一千个函数实例。如果你需要超过它，你可能会开始获得异常，排队和/或一般减速。 这里的问题是这个限制是在整个AWS账户中。有些组织使用相同的AWS账户进行生产和测试。这意味着如果组织中的某个人在您的组织中执行新类型的负载测试并开始尝试执行一千个并发的Lambda函数，那么您将意外地对生产系统发起DoS攻击。Oops。 即使您使用不同的AWS账户进行生产和开发，一个重载的生产环境的lambda（例如，处理来自客户的批量上载）也可能导致您的单独的实时lambda支持的生产系统API无响应。 亚马逊在这里做了一些保护，通过保留并发的方式（by way of reserved concurrency）。保留的并发性允许您限制Lambda函数的并发性，以便它不会炸毁您帐户的其余部分，同时确保无论帐户中的其他功能正在执行什么，总是有可用容量。但是，默认情况下，帐户不会启用保留并发，需要仔细管理。 执行时间（Execution duration）在文章的前面我提到AWS Lambda函数如果运行时间超过五分钟就会中止。这已经持续了几年，并且AWS没有显示出改变它的迹象。 启动延迟（Startup latency）我之前谈过冷启动，并提到了我关于这个主题的文章。 AWS随着时间的推移已经改进了这个领域，但是这里仍然存在重大问题，特别是对于偶尔触发的JVM实现的功能和/或需要访问VPC资源的功能。预计该领域将继续改善。 测试（Testing）单元测试Serverless应用程序非常简单，原因我之前已经讨论过：你编写的任何代码都是“只是代码”，并且大多数情况下你不必使用大量的自定义库或你需要实现的接口。 另一方面，集成测试Serverless应用程序很困难。在BaaS世界中，您有意依赖外部提供的系统，而不是您自己的数据库。那么，集成测试也应该使用外部系统吗？如果是，那么这些系统对测试场景的适应性如何？你能轻易地消除状态吗？您的供应商能否为负载测试提供不同的计费策略？ 如果你想要为外部系统的集成测试打桩，提供商提供的本地桩（local stub）模拟吗？如果是，这些桩的精确度好吗？如果提供商不提供桩你应该怎样自己实现？ FaaS领域也存在同样的问题，尽管这方面有所改善。现在可以在本地为Lambda和Microsoft Azure运行FaaS Function。但是，没有本地环境可以完全模拟云环境;完全依赖于本地FaaS环境并不是我推荐的策略。实际上，我会进一步建议，运行自动化集成测试的规范环境（至少作为deployment pipeline的一部分）应该是云，并且您应该使用本地测试环境主要用于交互式开发和调试。这些本地测试环境不断改进 - 例如，SAM CLI为开发Lambda支持的HTTP API应用程序提供快速反馈。 还记得我在云端运行集成测试时提到的几个部分以前的跨帐户执行限制吗？您可能希望至少将此类测试与生产云帐户隔离开来，并且可能使用比此更细粒度的帐户。 考虑集成测试是一件大事的部分原因是，我们与无服务器FAA（即每个功能）的集成单元比与其他架构的集成单元小得多，因此我们比与其他架构风格的集成测试依赖得更多。 依靠基于云的测试环境而不是在我的笔记本电脑上本地运行所有内容对我来说非常震惊。但是时代变了，我们从云端获得的功能与Google等工程师十多年来的相似。亚马逊现在甚至允许您在云中运行IDE。 调试（Debugging）使用FaaS进行调试是一个有趣的领域。这里取得了一些进展，主要与本地运行FaaS Function有关，与上面讨论的测试更新一致。正如我前面提到的，Microsoft为本地运行但由远程事件触发的函数提供了出色的调试支持。亚马逊提供类似的东西，但尚未被生产事件触发。 实际在生产云环境中运行的调试功能是另一回事。 Lambda至少对此没有支持，尽管看到这样的能力会很棒。 部署，打包和版本控制（Deployment, packaging, and versioning）这是一个正在积极改进的领域。 AWS在改进这一领域方面取得了巨大进步，稍后我将在“Serverless的未来”部分进一步讨论。 发现机制（Discovery）“发现”是微服务领域中经常讨论的主题：它是一个服务如何调用另一个服务的正确版本的问题。在Serverless世界中，几乎没有关于发现的讨论。最初这关心我，但现在我不那么担心了。Serverless的许多用法本质上是事件驱动的，并且在这里事件的消费者通常在某种程度上自我注册。对于面向API的FaaS用法，我们通常在API网关后面使用它们。在这种情况下，我们在API网关前使用DNS，并在网关后面进行自动部署/流量转移。我们甚至可以在API网关前面使用更多层（例如，使用AWS CloudFront）来支持跨区域弹性。 我将这个想法留在“limitation”中，因为我认为它还没有被证实，但最终它可能会很好。 监控与可观察性（Monitoring and observability）由于容器的短暂性，监控是FaaS的一个棘手问题。大多数云供应商都为您提供了一些监控支持，我们也从传统的监控供应商那里看到了很多第三方工作。尽管如此，无论他们和你最终能做什么，都取决于供应商为您提供的基本数据。在某些情况下这可能没问题，但对于AWS Lambda，至少它是非常基本的。我们在这方面真正需要的是开放API和第三方服务帮助更多的能力。 API网关定义和野心勃勃的API网关（API gateway definition, and over-ambitious API gateways）ThoughtWorks，作为它的Technology Radar出版物的一部分，讨论过野心勃勃的API网关(over-ambitious API gateways。）虽然链接通常指的是API网关（例如，对于那些前沿传统部署的微服务），但它绝对可以应用于将API网关用作FaaS函数的HTTP前端。问题在于，API网关提供了在自己的配置/定义域中执行许多特定于应用程序的逻辑的机会。这种逻辑通常很难测试、版本控制，有时还很难定义。通常，这样的逻辑与应用程序的其他部分一样保留在程序代码中要好得多。 不过这里肯定有紧张气氛。如果我们把一个API网关看作是一个BaaS，那么考虑它为我们提供的所有选项以节省我们的工作是不是很有价值？如果我们为每个请求使用API网关付费，而不是按CPU使用率付费，那么最大限度地使用API网关的功能是否更划算？ 我的指导思想是明智地使用增强的API网关功能，并且只有当它真正在长期内节省您的effort时，包括它的部署、监控和测试方式。绝对不要使用不能在源代码控制的配置文件或部署脚本中表达的API网关功能。 关于定义的困难，Amazon的API网关用于强制您创建一些复杂的配置，以将HTTP请求和响应映射到lambda函数或从lambda函数映射出来。在Lambda proxy integration中，大部分已经变得更简单了，但是您仍然需要了解一些偶尔棘手的细微差别。使用开源项目（如Serverless Framework 和 Claudia.js，或者Amazon的Serverless Application Model）可以使这些元素本身变得更容易。 Deferring of operations我之前提到过，Serverless不是“No Ops”——从监视、体系结构扩展、安全性和网络角度来看，还有很多工作要做。然而，当你开始的时候很容易忽略运维。这里的危险正在被一种虚假的安全感所麻痹。也许你的应用程序已经启动并运行了，但它却意外地出现在Hacker News上，突然间你有了10倍的流量需要处理，而且很糟糕！你不小心被DoS攻击了，却不知道怎么处理。 这里的解决办法是教学。使用Serverless系统的团队需要尽早考虑运维，供应商和社区将提供教学，帮助他们理解这意味着什么。诸如先发制人的负载测试和混沌工程（chaos engineering）等领域也将帮助团队自学。 Serverless的未来（The Future of Serverless）我们即将结束这个Serverless架构世界的旅程。最后，我将讨论一些我认为Serverless世界可能在未来几个月和几年内发展的领域。 改善缺陷（Mitigating the drawbacks）Serverless仍然是一个全新的世界。因此，前一节关于缺点的内容非常广泛，我甚至没有涵盖我所能拥有的一切。Serverless的最重要的发展将是减轻固有的缺点，消除或至少改善实现缺点。 工具（Tooling）工具仍然是Serverless的一个问题，这是因为许多科技和技术都是新的。部署、应用程序绑定和配置在过去两年中都得到了改进，Serverless框架和Amazon的Serverless应用程序模型引领了这一进程。然而，“最初的10分钟”的体验并不像它可能的那样令人惊讶，尽管亚马逊和谷歌可以寻求微软和Auth0的更多灵感。 我很高兴看到云供应商积极解决的一个领域是更高级别的发布方法。在传统系统中，团队通常需要编写自己的流程来处理“traffic-shifting”的想法，如蓝绿部署和金丝雀发布(canary releases）。考虑到这一点，亚马逊支持Lambda和API Gateway的automatic traffic shifting。这些概念在Serverless系统中甚至更有用，在这些系统中，如此多的单独部署的组件一次构成100个Lambda函数的系统原子发布是根本不可能的。事实上，Nat Pryce向我描述了一个“mixing desk”方法的想法，我们可以逐步将一组组件引入和移除流量。 分布式监控可能是需要最显着改进的领域。我们已经看到亚马逊的X-Ray和各种第三方产品的早期工作，但这绝对不是一个已经解决的问题。 远程调试也是我希望看到更广泛的东西。 Microsoft Azure Functions支持此功能，但Lambda不支持。能够断点远程运行函数是一项非常强大的功能。 最后，我希望看到“元操作”工具的改进——如何更有效地处理成百上千个FaaS函数、配置的服务等。例如，组织需要能够看到某些服务实例何时不再使用（出于安全目的，如果没有其他的话），它们需要更好地对以及跨服务成本的可视性（尤其是对于具有成本责任的自主团队），等等。 状态管理（State management）对于大量应用程序而言，FaaS缺乏持久的服务器内状态是好的，但对于许多其他应用程序而言，这是一个交易破坏者 - 无论是大型缓存集合还是快速访问会话状态。 高吞吐量应用程序的一种解决方法可能是供应商在事件之间保持函数实例的活动时间更长，并让常规的进程内缓存方法完成它们的工作。对于每个事件，缓存都不会变热，因此这不会100％有效，但这与使用自动缩放的传统部署应用程序已经存在的问题相同。 更好的解决方案是对进程外数据进行非常低延迟的访问，例如能够以非常低的网络开销查询Redis数据库。鉴于亚马逊已经在他们的Elasticache产品中提供托管的Redis解决方案，并且他们已经允许使用Placement Groups相对共享位置的EC2（服务器）实例，这似乎没有太大的影响。 不过，更可能的是，考虑到外部化的状态约束，我认为我们将看到不同类型的混合（Serverless和非Serverless）应用程序体系结构。例如，对于低延迟应用程序，您可能会看到一种常规、长时间运行的服务器处理初始请求的方法，从本地和外部状态收集处理该请求所需的所有上下文，然后将完全上下文化的请求传递给不需要从外部查找数据的FaaS函数场。 平台改进（Platform improvements）Serverless FaaS的某些缺点现在归结为平台的实现方式。执行持续时间，启动延迟和cross-function限制是三个显而易见的问题。这些可能会通过新的解决方案或可能的额外成本给出变通方法来解决。例如，我想通过允许客户请求FaaS函数的两个实例始终以低延迟可用，并且客户为此可用性付费，可以减轻启动延迟。 Microsoft Azure Functions具有此功能的元素，包括Durable Functions和App Service plan-hosted functions。 当然，除了修正当前的缺陷之外，我们还会看到平台改进，这些也将是令人兴奋的。 教育（Education）许多特定于供应商的Serverless固有缺陷正在通过教育得到缓解。使用此类平台的每个人都需要积极思考，让一个或多个应用程序供应商托管如此多的生态系统意味着什么。我们需要考虑这样的问题：“如果一个供应商不可用，我们是否要考虑来自不同供应商的并行解决方案？”和”在部分中断的情况下，应用程序如何正常降级？“ 另一个教育领域是技术运维。许多团队现在拥有的系统管理员数量比以前少，而Serverless将加速这一变化。但是系统管理员所做的不仅仅是配置Unix盒子和Chef脚本 - 他们通常是支持，网络，安全等一线的人。 在一个Serverless的世界里，真正的DevOps文化变得更加重要，因为其他非系统管理活动仍然需要完成，而且通常是开发人员现在负责这些活动。这些活动可能不会自然而然地出现在许多开发人员和技术领导面前，因此教育和与运维人员的密切合作至关重要。 提高供应商的透明度和更清晰的期望（Increased transparency and clearer expectations from vendors）最后，关于这个主题的改善：供应商必须更加清楚我们对其平台的期望，因为我们依赖它们来获得更多的托管功能。虽然迁移平台很难，但并非不可能，而且不值得信任的供应商会看到他们的客户将业务转移到其他地方。 模式的出现（The emergence of patterns）我们对如何以及何时使用Serverless架构的理解仍处于起步阶段。现在，团队正在Serverless平台上抛出各种想法并看到了什么。谢天谢地！我们开始看到推荐实践的模式发生，这种知识只会增长。 我们看到的一些模式是应用程序架构。例如，FaaS功能在变得笨拙之前有多大？假设我们可以原子地部署一组FaaS函数，那么创建此类分组的好方法是什么？他们是否密切关注我们当前如何将逻辑集成到微服务中，或者架构上的差异是否会将我们推向不同的方向？ Serverless应用程序架构中一个特别有趣的讨论领域是它如何与事件思维交互。 AWS Lambda产品负责人Ajay Nair在2017年对此进行了精彩的讨论，这是CNCF Serverless Working Group的主要讨论领域之一。 进一步扩展这一点，在FaaS和传统的“always on”持久服务器组件之间创建混合架构的好方法是什么？将BaaS引入现有生态系统的好方法是什么？而且，相反，完全或大部分BaaS系统需要开始采用或使用更多自定义服务器端代码的警告信号是什么？ 我们还看到了更多的使用模式被讨论。FaaS的一个标准示例是媒体转换，例如，每当一个大型媒体文件存储到S3存储桶中时，就会自动运行一个进程，在另一个存储桶中创建较小的版本。然而，我们现在也看到了Serverless在数据处理管道、高度可扩展的Web API以及在操作中作为通用“粘合”代码的大量使用。其中一些模式可以作为通用组件实现，可以直接部署到组织中；我已经写过关于Amazon的Serverless应用程序存储库的文章 ，它有一个早期的形式。 最后，随着工具的改进，我们开始看到推荐的操作模式。我们如何在逻辑上聚合FaaS，BaaS和传统服务器的混合架构的日志记录？我们如何最有效地调试FaaS功能？这些问题的许多答案 - 以及新兴模式 - 来自云供应商本身，我希望这一领域的活动能够增长。 全球性的的分布式架构（Globally distributed architectures）在我前面给出的宠物店示例中，我们看到单个宠物店服务器被分解为几个服务器端组件和一些逻辑，这些逻辑一直向上移动到客户端，但从根本上讲，这仍然是一个架构，重点是客户端或已知位置的远程服务。 在Serverless的世界里，我们现在看到的是更加模糊的责任分配。一个例子是亚马逊的lambda@edge产品：一种在亚马逊的CloudFront内容交付网络中运行lambda函数的方法。对于Lambda@Edge，lambda函数现在是全局分布的——工程师的单个上载活动意味着该函数将部署到全球100多个数据中心。这不是我们习惯的一种设计，它带有许多约束和功能。 此外，lambda函数可以在设备上运行，机器学习模型可以在移动客户端上运行，在您了解它之前，“客户机端”和“服务器端”的分支似乎不再有意义。事实上，我们现在看到的是组件的局部性，从人类用户那里扩散开来。Serverless将变为无区域的。 不仅仅是FaaSification（Beyond “FaaSification”）到目前为止，我所看到的faas的大多数用法主要是利用现有的代码和设计思想，并将它们“简化”：将它们转换为一组无状态函数。这是很强大的，但是我希望我们会看到更多的抽象，可能还有语言，使用FaaS作为底层实现，让开发人员在不考虑将其应用程序作为一组离散函数的情况下获得FaaS的好处。 例如，我不知道Google是否为其Dataflow产品使用FaaS实现，但我可以想象有人创建了一个类似的产品或开源项目，并使用FaaS作为实现。这里的比较类似于Apache Spark。 Spark是一种用于大规模数据处理的工具，它提供了非常高级的抽象，可以使用Amazon EMR and Hadoop作为其底层平台。 测试（Testing）我认为在Serverless系统的集成和验收测试方面还有很多工作要做，但是很多工作与以传统方式开发的“云原生”微服务系统相同。 这里的一个激进想法是接受诸如testing in production和monitoring-driven development之类的想法;一旦代码通过了基本的单元测试验证，就可以部署到流量子集并查看它与之前版本的比较情况。这可以与我前面提到的traffic-shiting工具相结合。这并不适用于所有情况，但对于许多团队来说，它可能是一个令人惊讶的有效工具。 便携式的实现（Portable implementations）团队可以通过几种方式使用Serverless，同时减少与特定云供应商的联系。 夸供应商实现的抽象（Abstractions over vendor implementations）Serverless Framework主要用于简化Serverless应用程序的操作任务，但也提供了关于部署此类应用程序的位置和方式的中立性。例如，根据每个平台的操作功能，即使在AWS API Gateway + Lambda和Auth0 webtask之间轻松切换也很容易。 这方面的一个棘手问题是对抽象的FaaS编码接口进行建模而没有一些标准化的想法，但这恰恰是CNCF Serverless Working Group在CloudEvents上的工作。 一旦复杂的操作让人头皮发麻（once complexities of operations rear their ugly heads），那么为多个云平台提供部署抽象是否还有意义。比如：获取一个云的安全权限往往和其他云是不同的。 可部署的实现（Deployable implementations）建议我们在不使用第三方提供商的情况下使用Serverless技术可能听起来很奇怪，但请考虑以下想法： 也许我们是一个大型技术组织，我们希望开始为所有移动应用程序开发团队提供类似Firebase的数据库体验，但我们希望使用现有的数据库架构作为后端。 我之前谈过“Serverful”FaaS平台 - 能够在我们的一些项目中使用FaaS风格的架构，但提交合规，法律等原因来在我们的非云环境（on premise）运行我们的应用程序。 在这两种情况下，使用Serverless而不使用来自供应商托管的方法仍然有许多好处。这里有一个先例，将平台视为服务（paas）。最初流行的PaaS都是基于云的（如Heroku），但是，很快，人们就看到了在自己的系统上运行PaaS环境的好处——所谓的“私有”PaaS（如本文前面提到的Cloud Foundry）。 我可以想象，就像私有PaaS实现一样，我们会看到BaaS和FaaS概念的开源和商业实现变得流行，特别是与Kubernetes等容器平台集成的那些。 Community（社区）已经有一个规模很大的Serverless社区，其中包括多个会议，许多城市的聚会以及各种在线群组。我预计这种情况会持续增长，可能与Docker和Spring等社区同样如此。 Conclusion（结论）尽管名称令人困惑，但Serverless是一种架构风格，我们依赖于运行我们自己的服务器端系统作为我们应用程序的一部分，其程度比平时要小。我们通过两种技术实现这一目标：BaaS，我们将第三方远程应用程序服务直接集成到我们应用程序的前端，以及FaaS，它将服务器端代码从长时间运行的组件移动到短暂的函数实例。 Serverless不是对于每个问题都是正确的方法，所以要警惕那些说它将取代所有现有架构的人。如果您现在投入Serverless系统，请特别注意，尤其是在FaaS领域。虽然有强大伸缩功能并且节省了发部分的部署effort，但是调试和监控任然存在挑战。 然而，这些财富不应该太快地被抛弃，因为Serverless架构有很多积极的方面，包括降低运维和开发成本、更容易的运维管理和减少环境影响。但我认为最重要的好处是减少了创建新应用程序组件的反馈循环。我非常喜欢“精益求精”方法，这主要是因为我认为面对终端用户，我们应该尽快掌握技术以获得早期反馈很有价值，而Serverless的上市时间缩短正好符合这一理念。 Serverless服务，以及我们对如何使用它们的理解，（2018年5月）其仍处于“略显尴尬的青少年时期”。未来几年，该领域将取得许多进展，看看Serverless如何融入我们的架构工具包将是非常有趣的。]]></content>
      <categories>
        <category>serverless</category>
      </categories>
      <tags>
        <tag>serverless</tag>
        <tag>cloud native</tag>
        <tag>faas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译：Kubernetes: Lifecycle of a Pod]]></title>
    <url>%2F2018%2F08%2F29%2F%E7%BF%BB%E8%AF%91%EF%BC%9AKubernetes-Lifecycle-of-a-Pod_new%2F</url>
    <content type="text"><![CDATA[翻译： Kubernetes: Lifecycle of a Pod查看Kubernetes pod的生命周期可以帮助了解可以在流行的容器软件中调度的最小工作单元发生了什么。 在Kubernetes中，Pod是能被调度的最小工作单元。一个Pod封装了一个应用的容器，存贮资源，唯一的网络IP和管理容器运行的其他选项。理想情况下，Pod并不会被集群直接部署，而是用更高层次的抽象层。应用通常通过更高层的资源对象部署，比如：Deployments, Replication Sets, Daemon Sets, Stateful Sets或者Jobs。和Pod进行交互主要用来解决问题，因此深入Pod是很重要的。 Pod的状态整个Pod的声明周期中，可以达到以下状态： Pending：Pod被Kubernetes系统接收（accepted），但是没有其容器还没有被完全创建。 Running：Pod被调度到某一个Node上，所有的容器都被创建并且至少有一个容器是Running状态。 Succeeded：Pod中所有的容器都以状态0退出并且不会重启。 Failed：Pod中所有的容器都已经退出，但是至少有一个容器退出状态不为0。 CrashLoopBackoff：容器启动失败并且会被不断重试。 容器的诞生现在我们来看看导致一个容器创建的所有事件。 kubectl或者其他API客户端提交Pod spec到API server。 API server会将Pod对象写到etcd数据存储中。一旦写操作成功，一个ack将会被发送回API server和客户端。 现在API server反映了etcd的状态变化。 所有的Kubernetes组件通过watch API保持对API server相关变化的检测。 当前情况下，kube-scheduler（通过watcher）发现一个新的Pod通过API server被创建，但是还没有绑定到任何一个node。 Kube-scheduler会给当前新创建的Pod分配一个node，并跟新API server。 “Pod被分配到某个节点”这个变化将会被传递给etcd数据存储。API server也会在其Pod project上体现出节点分配。 每个节点上的Kubelet也会运行watcher，对API server持续监控。在目标node上，kubelet知道一个新的Pod已经分配。 Kubelet在node上通过调用Docker启动Pod，并且将跟新的容器状态发送回API server。 API server将Pod状态持久化到etcd中。h 一旦etcd发送成功写操作的ack到API server，API server 会回复Kubelet一个ack，表示事件已经被接收。 Pod生命周期中的活动初始容器初始容器是运行在主应用程序启动之前的容器。他们有两个重要的特征： 总是运行至完成状态 每一个初始容器都必须在下一个启动之前完成 初始容器能在主容器启动之前做一些必要的初始化操作。 比如：拷贝配置文件和修改配置。初始容器使用不同的Linux命名空间，所以他们有不同的文件系统视图，所以他们可以被允许访问可能不适合在主应用程序内共享的秘密。 Lifecycle Hookskubelet 可以运行被Container Lifecycle Hooks触发的代码。这允许用户在容器生命周期指定事件期间运行指定的代码。 存在两个被暴露出来的hook： PostStart：这种hook在容器后立即运行，但是没有办法保证在容器的ENTRYPOINT之后运行。 PreStop：这种hook在容器终止前被执行，是阻塞的，意味着必须在删除容器的调用之前完成hook的执行。 上面提到的两种hook都不能带参数。 容器可以通过实现的和注册该hook的handler来访问hook。有两种类型的hook handler可以被容器应用： Exec：在Container的cgroups和名称空间内执行特定命令，例如pre-stop.sh，命令执行的资源消耗算在容器内。 HTTP：对容器上指定的endpoint执行HTTP请求。 容器探针除了Lifecycle hooks之外，在Pod生命周期中发生的另一个重要事情就是容器探针的执行。 容器探针是由容器上的kubelet执行的诊断。 kubelet可以在运行容器上运行两种探针： livenessProbe：指示容器是否正在运行。如果活动探测失败，则kubelet会杀死容器，并且容器会受到其重新启动策略的影响。 readnessProbe：指示容器是否已准备好为请求提供服务。如果此探测失败，则endpoint controller将从与Pod匹配的所有服务的端点列表中删除容器IP。 有三种方式实现一个探针： ExecAction：在容器内部执行命令。如果命令返回0，则为诊断成功。 TCPSocketAction：对容器IP和指定端口执行TCP套接字检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction：使用指定的端口和路径对容器IP执行HTTP GET操作。如果响应的状态代码在200到400之间，则认为诊断成功。 容器的终止 用户执行一个命令删除一个Pod API服务器中的Pod对象将随着Pod被认为“死亡”（默认为30秒）的时间以及宽限期而更新。 下面的行为将会并行执行： 在客户端命令中列出时，状态为“Terminating”。 当Kubelet看到因为设置了第二点中的时间而将Pod标记为终止时，它开始pod关闭过程。 端点控制器监视即将删除的pod，因此从该pod提供服务的所有端点中删除该pod。 如果Pod已经定义了preStop hook，并且在Pod内被调用。如果preStop hook在Grace time超时后，任然为运行状态，然后用小的（2秒）扩展宽限期调用步骤2。 Kubelet向Docker发送TERM信号。 当Grace period超时，任何运行在Pod上的进程都会被SIGKILL杀死。 Kubelet将通过设置宽限期0（立即删除）完成删除API服务器上的Pod。 Pod从API中消失，不再从客户端可见。 总结这篇文章的想法来自Kubernetes创始人Joe Beda的精彩文章，他解释了Kubernetes架构的主要组成部分和watch概念，这对于理解APIServer如何工作和etcd功能以及Pod的诞生至关重要。 我们可以看到有多种方式来控制在POD的生命周期内发生的事件。初始容器可以帮助移除与容器引导相关的许多复杂性，从而帮助保持主容器内的逻辑简单。类似地，启动preStart lifecycle hook可以帮助运行容器启动后需要运行的任何代码（例如向监视系统或服务网格注册）。liveness probe和readness probe有助于在开始危害任何客户之前移除坏的Pod。优雅的shutdown可以作为一个pre-stop lifecycle hook运行，允许更优雅的退出。了解上述控制机制有助于更好地设计Pod和支持使用案例。 原文出处：https://dzone.com/articles/kubernetes-lifecycle-of-a-pod 参考：https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>pod</tag>
        <tag>lifecycle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用JWT访问Kubernetes API-server REST API]]></title>
    <url>%2F2018%2F08%2F29%2F%E4%BD%BF%E7%94%A8JWT%E8%AE%BF%E9%97%AEKubernetes-API-server-REST-API_new%2F</url>
    <content type="text"><![CDATA[使用JWT访问Kubernetes API-server REST API生成Token首先在kubernetes集群的master节点上获取API-Server 的地址： 1echo $(kubectl config view | grep server | cut -f 2- -d ":" | tr -d " ") 在集群的master结点上，运行以下指令，获取具有所有权限的token值： 创建admin-token.yaml，并输入以下内容 1234567891011121314151617181920212223apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: "true"roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects: - kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile 执行命令：kubectl create -f admin-token.yaml 执行命令：kubectl get secret -n kube-system|grep admin 执行命令：kubectl describe secret admin-token-sdv88 -n kube-system 可以使用上面生成的JWT来访问API-Server。 通过Postman访问 使用RestTemplate访问，配置RestTemplate需要跳过Client对API-Server的认证。 123456789101112131415161718192021222324252627282930313233@GetMapping("/test") public String testK8sApi() throws KeyStoreException, NoSuchAlgorithmException, KeyManagementException &#123; HttpHeaders headers = new HttpHeaders(); headers.set("Authorization", "Bearer " + "eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1zZHY4OCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjlhMzM5NzI1LWE0M2YtMTFlOC1iZGJmLTAwNTA1NmE0MDg0MCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.FxoNIIrlVvn0EV5HqTwQh--QO-gL-MTEz6BXvY9hsXEUSe-HuWgwb4cokPGAxEavPLafrbEK8YiL66fT3C-xH0T43my4HN_njJEWMwmBBiFU-G5B5QwMbP7WTSXTtrpkeyQM0G8wysWFhsUsper5Ke-xX8I0eY3CBOxeMchsO7QAd-py13h_ufXVKoPZvz5wAjF_5sLMRiJfpG5FKjOTe9OgCLNCr7yCrHMvcr-TvMVcfja3Eyv3lOJIjes2jpCOwXH8xRJ-dRS0CpPfiXgYDDqgwK3SjTQ7fTOIQgZNBsmRNYmcCPWKQvuQMOlVGwf1ozlaOZmyGggvirSWKJUOlA"); TrustStrategy acceptingTrustStrategy = (X509Certificate[] chain, String authType) -&gt; true; SSLContext sslContext = org.apache.http.ssl.SSLContexts.custom() .loadTrustMaterial(null, acceptingTrustStrategy) .build(); SSLConnectionSocketFactory csf = new SSLConnectionSocketFactory(sslContext); CloseableHttpClient httpClient = HttpClients.custom() .setSSLSocketFactory(csf) .build(); HttpComponentsClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(); requestFactory.setHttpClient(httpClient); RestTemplate restTemplate = new RestTemplate(requestFactory); HttpEntity&lt;String&gt; entity = new HttpEntity&lt;String&gt;(headers); ResponseEntity&lt;String&gt; responseEntity = restTemplate.exchange("https://10.141.212.142:6443", HttpMethod.GET, entity, String.class); JsonParser parser = new JsonParser(); JsonObject json = parser.parse(responseEntity.getBody()).getAsJsonObject(); Gson gson = new GsonBuilder().setPrettyPrinting().create(); String prettyJson = gson.toJson(json); return prettyJson; &#125;]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>JWT</tag>
        <tag>API-Server</tag>
        <tag>Authentication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7搭建kubernetes集群]]></title>
    <url>%2F2018%2F08%2F16%2FCentOS-7%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4_new%2F</url>
    <content type="text"><![CDATA[CentOS 7 搭建Kubernetes集群安装Dockeryum install -y docker systemctl enable docker &amp;&amp; systemctl start docker 设置master node和work node的时间为了最后join成功，需要集群每个node之间时间保持一致，在每一台服务器上执行以下命令，指定时区。最后可以通过date命令查看设置是否生效。 1234567yum install -y chronysystemctl start chronydsystemctl enable chronydtimedatectl set-timezone Asia/Shanghaitimedatectl set-ntp yesdate 安装kubeadm参考自：kubeadm官方安装教程 12345678910111213141516171819cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgexclude=kube*EOFsetenforce 0yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetessystemctl enable kubelet &amp;&amp; systemctl start kubeletcat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 初始化Master节点安装好kubeadm之后，在master node上执行kubeadm init命令： kubeadm init --pod-network-cidr=10.244.0.0/16 可能出现kubeadm init报以下错： running with swap on is not supported. Please disable swap 执行命令disable swap：sudo swapoff -a Note 每次重新init之前需要执行命令：kubeadm reset，清空之前的残留。 若出现&quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;。错误，原因是docker和kubelet的cgroup不一致导致的（改docker或者kubelet的cgroup）。解决方案(install docker部分) master节点重启后必须执行： 12sysctl net.bridge.bridge-nf-call-iptables=1sudo swapoff -a kubeadm init命令执行成功之后，会出现以下提示消息： 1234You can now join any number of machines by running the following on each nodeas root: kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 上述命令用于后面work节点的join操作。 如果不是Root用户，可以执行以下命令让你的kubectl生效： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 如果你是Root用户，直接执行以下命令，使kubectl生效： 1export KUBECONFIG=/etc/kubernetes/admin.conf 为了不让终端断开之后每次都执行一次，可以在/etc/profile文件末尾加上export KUBECONFIG=&quot;/etc/kubernetes/admin.conf&quot;，然后source /etc/profile。 安装Pod 网络插件必须安装Pod网络插件才能使Pod之间能正常通信，这里我们使用Flannel插件。 为了让Flannel正常工作，必须注意： 设置kubeadm init的运行参数为--pod-network-cidr=10.244.0.0/16（前面已经设置）。 执行sysctl net.bridge.bridge-nf-call-iptables=1 安装Flannel： 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml 一旦Pod network安装成功，可以使用kubectl get pods —all-namespaces来查看CoreDNS pod状态是否为running，如果是，则表示可以join其他work nodes。如果不是running状态，参考troubleshooting。 Join work node为了更加直观的反应节点名称，可以修改每个node的/etc/hosts文件，比如Master node的ip为10.141.211.163，设置host name为CENTOS-MASTER： 12310.141.211.163 CENTOS-MASTER10.141.211.164 CENTOS-MINION-110.141.211.177 CENTOS-MINION-2 然后reboot所有的节点，master节点重启后必须执行： 12sysctl net.bridge.bridge-nf-call-iptables=1sudo swapoff -a 使用kubeadm joinwork node加入集群： 使用ssh连接远端服务器 切换root用户(sudo su -) 执行kubeadm join命令，实例： 1kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 第3步中的命令可以在kubeadm init后，在控制台输出中找到。 Note 如果没有token可以在master节点使用kubeadm token list获取。 token默认的有效期为24小时，如果过期，可以在master节点使用kubeadm token create生成新的token。 如果没有—discovery-token-ca-cert-hash可以使用openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \ openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39;获取。 至此，集群搭建就成功了，可以在Master节点查看kubectl get nodes。 拆除集群要撤消kubeadm所执行的操作，您应首先排空节点，并确保节点在关闭之前为空。在Master节点执行： 12kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 在对应的work node上执行： 1kubeadm reset 集群维护其他插件]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>cluster</tag>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes service配置详解]]></title>
    <url>%2F2018%2F08%2F13%2Fkubernetes-service%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3_new%2F</url>
    <content type="text"><![CDATA[kubernetes service配置详解以下配置信心归纳自《Kubernetes权威指南》第二版。123456789101112131415161718192021222324252627282930apiVersion: v1 # string Required 版本kind: Service # string Required 资源类型metadata: # Object Required 元数据 name: string # string Required Service名称 namespace: string # string Required 命名空间，默认default labels: # list Required 自定义标签属性列表 - name: string annotations: # list Required 自定义注解属性列表 - name: stringspec: # Object Required 详细描述 selector: [] # list Required Label Selector配置，将选择具有指定Label标签的Pod作为管理范围 type: string # string Required Service的类型，指定Service的访问方式，默认为ClusterIP # ClusterIP：虚拟的服务IP地址，改地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置iptables规则进行转发 # NodePort: 使用宿主机的端口，使能够访问个Node的外部客户端通过Node的IP地址和端口号就能访问服务 # LoadBalancer: 使用外接负载均衡完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址， # 并同时定义nodePort和clusterIP，用于公有云环境 clusterIP: string # string 虚拟服务IP地址，当type=ClusterIP时，如果不指定，则系统自动分配，也可以手动指定；当type=LoaderBalancer需要指定 sessionAffinity: string # string 是否支持Session，可选值为ClientIP，默认为空 # ClientIP: 表示将同一个客户端（根据IP地址决定）的访问请求都转发到同一个后端Pod ports: # list 需要暴露的端口列表 - name: string # 端口名称 protocol: string # 端口协议 支持tcp和udp，默认为tcp port: int # 服务监听的端口号 targetPort: int # 需要转发到后端Pod的端口号 nodePort: int # 当spec.type=NodePort时，指定映射到物理机的端口号 status: # 当spec.type=LoadBalancer时，设置外部负载均衡器的地址，用于公有云环境 loadBalancer: # 外部负载均衡器 ingress: # 外部负载均衡器 ip: string # 外部负载均衡器的IP地址 hostname: string # 外部负载均衡器的主机名]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes service配置详解]]></title>
    <url>%2F2018%2F08%2F13%2Fkubernetes-service%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3_new_new%2F</url>
    <content type="text"><![CDATA[kubernetes service配置详解以下配置信心归纳自《Kubernetes权威指南》第二版。123456789101112131415161718192021222324252627282930apiVersion: v1 # string Required 版本kind: Service # string Required 资源类型metadata: # Object Required 元数据 name: string # string Required Service名称 namespace: string # string Required 命名空间，默认default labels: # list Required 自定义标签属性列表 - name: string annotations: # list Required 自定义注解属性列表 - name: stringspec: # Object Required 详细描述 selector: [] # list Required Label Selector配置，将选择具有指定Label标签的Pod作为管理范围 type: string # string Required Service的类型，指定Service的访问方式，默认为ClusterIP # ClusterIP：虚拟的服务IP地址，改地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置iptables规则进行转发 # NodePort: 使用宿主机的端口，使能够访问个Node的外部客户端通过Node的IP地址和端口号就能访问服务 # LoadBalancer: 使用外接负载均衡完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址， # 并同时定义nodePort和clusterIP，用于公有云环境 clusterIP: string # string 虚拟服务IP地址，当type=ClusterIP时，如果不指定，则系统自动分配，也可以手动指定；当type=LoaderBalancer需要指定 sessionAffinity: string # string 是否支持Session，可选值为ClientIP，默认为空 # ClientIP: 表示将同一个客户端（根据IP地址决定）的访问请求都转发到同一个后端Pod ports: # list 需要暴露的端口列表 - name: string # 端口名称 protocol: string # 端口协议 支持tcp和udp，默认为tcp port: int # 服务监听的端口号 targetPort: int # 需要转发到后端Pod的端口号 nodePort: int # 当spec.type=NodePort时，指定映射到物理机的端口号 status: # 当spec.type=LoadBalancer时，设置外部负载均衡器的地址，用于公有云环境 loadBalancer: # 外部负载均衡器 ingress: # 外部负载均衡器 ip: string # 外部负载均衡器的IP地址 hostname: string # 外部负载均衡器的主机名]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes Pod的扩容和伸缩]]></title>
    <url>%2F2018%2F08%2F07%2Fkubernetes-Pod%E7%9A%84%E6%89%A9%E5%AE%B9%E5%92%8C%E4%BC%B8%E7%BC%A9_new%2F</url>
    <content type="text"><![CDATA[Kubernetes Pod的扩容和伸缩此笔记来自于《Kubernetes权威指南：从Docker到Kubernetes实践全接触（第二版）》2.4.9节，由于Kubernetes的版本的不断迭代跟新，书中有些教程由于版本原因，不在适用于高版本k8s，本笔记在原著教程基础上做了适当调整，已适用于目前版本的K8s。 通过kubectl scale命令首先创建expansion-test.yaml文件。1234567891011121314151617apiVersion: v1kind: ReplicationControllermetadata: name: expansion-testspec: replicas: 2 selector: test: expansion template: metadata: labels: test: expansion spec: containers: - name: expansion-test image: tomcat imagePullPolicy: IfNotPresent 执行命令kubectl create -f expansion-test.yaml 如上图所示，已经成共创建了一个名为expansion-test的ReplicationController，使用kubectl get pod命令查看创建的Pod，一共有两个，因为spec.replicas=2。 现在使用kubectl scale rc expansion-test —replicas=4命令，将expansion-test的副本数增加至4。 然后查看Pod数量，已经增加至4。 基于CPU使用率的Pod自动扩容和伸缩Kubernetes v1.1增加了Horizontal Pod Autoscaler(HPA)控制器，用于实现CUP利用率的Pod扩缩容功能。HPA控制器基于Master的kube-controller-manager服务启动参数—horizontal-pod-autoscaler-sync-period定义的时长（默认为30秒），周期性的检测目标Pod的CPU使用率，并在满足条件时对ReplicationController或Deployment中的Pod数量进行调整，以符合用户定义的平均Pod CPU使用率。Pod CPU使用率来源于heapster组件，所以需要预先安装好heapster。 创建HPA时可以用kubectl scale命令进行快速创建或者使用yaml配置文件进行创建。在创建HPA之前，需要存在一个RC或者Deployment对象，并且该RC或Deployment中的Pod必须定义resources.requests.cpu的资源请求值，如果不设置该值，则heapster将无法采集到该Pod的CPU使用情况，会导致HPA无法正常工作。 安装heapster组件：git clone git@github.com:kubernetes/heapster.git kubectl create -f ./deploy/kube-config/influxdb/ 创建上述的Deployment时，需要用到 k8s.gcr.io/heapster-amd64:v1.5.4 k8s.gcr.io/heapster-grafana-amd64:v5.0.4 k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 三个镜像，因此执行上面第二条命令需要配置docker代理，当然读者也可以通过Alibaba、DaoCloud等提供的镜像仓库中获取。 创建ReplicationController因为创建HPA之前，要先存在一个RC或者Deployment，并且必须在该RC或Deployment中的Pod必须定义resources.requests.cpu的资源请求值，所以这里以RC为例，其requests.cpu=200m，没有limits限制。 首先创建php-apache-rc.yaml配置文件。 1234567891011121314151617181920apiVersion: v1kind: ReplicationControllermetadata: name: php-apchespec: replicas: 1 template: metadata: name: php-apache labels: app: php-apache spec: containers: - name: php-apache image: gcr.io/google_containers/hpa-example resources: requests: cpu: 200m ports: - containerPort: 80 创建RC: kubectl create -f php-apache-rc.yaml 上图status为imagePullBackOff，可以使用kubectl logs命令查看日志，原因是被qiang了。因为pull gcr.io/google_containers/hpa-example镜像需要配置docker代理。这里可以自己百度一下docker代理配置，很简单~ 配置好代理之后，删除当前Pod，由于rc中replicas=1，所以rc会自动重新创建一个新的Pod，当status为running时，Pod创建成功。 创建Service创建一个php-apache的Service，供客户端访问。首先先创建如下php-apache-svc.yaml。 123456789apiVersion: v1kind: Servicemetadata: name: php-apachespec: ports: - port: 80 selector: app: php-apache 执行kubectl get svc创建Service，然后查看创建情况，创建成功。 创建HPA控制器创建HPA时可以用kubectl scale命令进行快速创建或者使用yaml配置文件进行创建。 使用kubectl scale命令创建HPA可以使用以下命令来创建HPA，最大Pod数量为10，最小Pod数量为1，平均CPU利用率维持在50%。 kubectl autoscale rc php-apache —min=1 —max=10 —cpu-percent=50 使用yaml配置文件创建HPA首先先创建如下图所示的hpa-php-apache.yaml文件。 12345678910111213apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: hpa-php-apache namespace: defaultspec: scaleTargetRef: apiVersion: v1 kind: ReplicationController name: php-apache minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 创建HPA并查看结果。 如上图所示，使用kubectl get hpa命令查看创建情况，发现 target一栏中使用的CPU为\&lt;unknown>，说明HPA从heapster获取CPU数据失败，这是因为k8s 1.9 之前kube-controller-manager 参数--horizontal-pod-autoscaler-use-rest-clients为false，1.9之后true。当然，最简单的方式就是修改其值为false，具体详见官方issue。 更好的办法是安装metrix server组件，步骤如下： git clone git@github.com:kubernetes-incubator/metrics-server.git 1.8以上：kubectl create -f deploy/1.8+/ 1.7或以下：kubectl create -f deploy/1.7/ 安装完之后，删除当前的HPA：kubectl delete hpa hpa-php-apache，重新创建：kubectl create -f hpa-php-apache.yml。 此时可以看到target那一栏\&lt;unknown>变为了0%说明当前的CPU使用率为0%。 压力测试接下来对php-apache service进行压测，首先先创建busybox-pod.yaml文件。 123456789apiVersion: v1kind: Podmetadata: name: busybox spec:containers: - name: busybox image: busybox command: [ "sleep", "3600" ] 然后执行kubectl create -f busybox-pod.yaml创建Pod。 然后登陆到上面创建的busybox中：kubectl exec -it busybox /bin/sh 然后在容器中执行while true; do wget -q -O- http://php-apache &gt; /dev/null; done，不断向php-apache Service发送请求，过一段时间后观察HPA搜集到PodCPU使用率。 kubectl get hpa 可以看出，CPU占用率不断升高，HPA通过不断增加Replicas来使CPU平均使用率保持在设定的50%。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>pod</tag>
        <tag>HPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod 配置详解]]></title>
    <url>%2F2018%2F08%2F02%2Fkubernetes-pod-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3_new%2F</url>
    <content type="text"><![CDATA[kubernetes pod 配置详解以下配置归纳自《Kubernetes权威指南》第二版。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182apiVersion: v1 # String, Required, 版本号kind: Pod # String, Required, Podmetadata: # Object, Required, 元数据 name: string # String, Required, Pod的名称 namespaces: string # String, Required, Pod所属命名空间，默认为default labels: # List, Not Required, 自定义标签列表 - name: string annotations: # List, Not Required, 自定义注解列表 - name: stringspec: # Object, Required, Pod中容器的详细定义 containers: # List, Required, Pod中的容器列表 - name: string # String, Required,容器名称 image: string # String, Required,容器的镜像名称 # String, Not Required, 获取镜像的策略,默认为Always: 每次都重新获取,Never: 仅使用本地镜像,IfNotPresent: 本地没有就下载 imagePullPolicy: [Always | Never | IfNotPresent] command: [string] # List, Not Required, 容器的启动命令，如果缺省，就使用容器打包时的启动命令 args: [string] # List, Not Required, 容器的启动命令参数列表 workingDir: string # String, Not Required, 容器的工作目录 volumeMounts: # List, Not Required, 挂载到容器内部的存储卷配置 - name: string # string, Not Required, 引用Pod定义的共享存储卷的名称， 需要用volumes[]部分定义的共享存储卷名称 mountPath: string # string, Not Required, 存储卷在容器内Mount的绝对路径，应少于512字符 readOnly: boolean # Boolean, Not Required, 是否为字符模式， 默认为读写模式 ports: # List, Not Required, 容器需要暴露的端口列表 - name: string # String, Not Required, 端口的名称 containerPort: int # Int, Not Required, 容器需要监听的端口号 hostPort: int # Int, Not Required, 不建议配置！！！容器所在主机需要监听的端口号， 默认与contianer相同。 设置hostPort时，同一台宿主主机上无法启动该容器的第二份副本。 protocol: string # String, Not Required, 端口协议，支持TCP和UDP，默认为TCP env: # List, Not Required, 容器运行前需要设置的环境变量列表 - name: string # string, Not Required, 环境变量的名称 value: string # string, Not Required, 环境变量的值 - name: string # string, Not Required, 环境变量的名称 valueFrom: # Object, Not Required, 环境变量来自哪里 configMapKeyRef: # Object, Not Required, 来自configMap name: string # string, Not Required, ConfigMap的名称 key: string # string, Not Required, configMap的键 resources: # Object, Not Required, 资源限制和资源需求设置 limits: # Object, Not Required, 资源限制的设置 cpu: string # string, Not Required, CPU的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string # string, Not Required, 内存的限制，单位可以为MiB/GiB等，将用于docker run --memory参数 requests: # Object, Not Required, 资源需求的设置 cpu: string # string, Not Required, CPU需求，单位为core数，容器启动的初始可用数量 memory: string # string, Not Required, 内存需求，单位可以为MiB,Gib等，容器启动时的初始可用数量 livenessProbe: # Object, Not Required, 对Pod内各个容器进行健康检查的设置，当探测无响应几次之后，系统会自动重启该容器。对一个容器仅需设置一种健康检查的方法。 exec: # Object, Not Required, 设置健康检查方式为exec方式 command: [string] # string, Not Required, 健康检查需要执行的命令或者脚本 httpGet: # Object, Not Required, 设置健康检查的方式为httpGet方式 path: string # string, Not Required, port: number # number, Not Required, host: string # string, Not Required, scheme: string # string, Not Required, httpHeaders: # Object, Not Required, - name: string # string, Not Required, value: string # string, Not Required, tcpSocket: # Object, Not Required, 设置健康检查方式为tcpSocket port: number # number, Not Required, initialDelaySeconds: 0 # number, Not Required, 容器启动完成后进行首次健康检查的时间，单位为秒 timeoutSeconds: 0 # number, Not Required, 健康检查等待响应超时时间，单位为秒，默认为1秒，超时则重启 periodSeconds: 0 # number, Not Required, 对容器健康检查的定期探测时间设置，单位为秒，默认为10秒探测1次 successThreshold: 0 # number, Not Required, failureThreshold: 0 # number, Not Required, securityContext: # 详见： https://www.kubernetes.org.cn/security-context-psp privileged: falserestartPolicy: [Always | Never | OnFailure]nodeSelector: object # Object, Not Required, 设置NodeSelector表示将该pod调度到包含这些label的Node上，以key: value形式指定imagePullSecrets: # Object, Not Required, pull镜像时使用的secret名称，以name: secretKey格式指定 - name: stringhostNetwork: false # Boolean, Not Required, 不建议配置！！！是否使用主机网络。默认为false，如果设置为true，不再使用docker网桥，该pod将无法再同一台宿主机上启动第二个副本volumes: # List, Not Required, 在该Pod定义的共享存储卷列表 - name: string # string, Not Required, 共享存储卷的名称。容器定义部分的container[].volumeMount[].name将引用此名称 emptyDir: &#123;&#125; # Object, Not Required, 类型为emptyDir的存储卷，表示与Pod生命周期相同的一个临时目录，其值为一个空对象 hostPath: # Object, Not Required, 类型为hostPath的存储卷，表示与挂载Pod所在宿主机的目录 path: string # string, Not Required, Pod所在主机的目录，将被用于容器中mount的目录 secret: # Object, Not Required, 类型为secret的存储卷，表示挂载集群预定义的secret对象到容器内部 secretName: string items: - key: string path: string configMap: # Object, Not Required, 类型为secret的存储卷， 表示挂载集群预定义的configMap对象到容器内部 name: string items: - key: string path: string]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod 配置详解]]></title>
    <url>%2F2018%2F08%2F02%2Fkubernetes-pod-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3_new_new%2F</url>
    <content type="text"><![CDATA[kubernetes pod 配置详解以下配置归纳自《Kubernetes权威指南》第二版。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182apiVersion: v1 # String, Required, 版本号kind: Pod # String, Required, Podmetadata: # Object, Required, 元数据 name: string # String, Required, Pod的名称 namespaces: string # String, Required, Pod所属命名空间，默认为default labels: # List, Not Required, 自定义标签列表 - name: string annotations: # List, Not Required, 自定义注解列表 - name: stringspec: # Object, Required, Pod中容器的详细定义 containers: # List, Required, Pod中的容器列表 - name: string # String, Required,容器名称 image: string # String, Required,容器的镜像名称 # String, Not Required, 获取镜像的策略,默认为Always: 每次都重新获取,Never: 仅使用本地镜像,IfNotPresent: 本地没有就下载 imagePullPolicy: [Always | Never | IfNotPresent] command: [string] # List, Not Required, 容器的启动命令，如果缺省，就使用容器打包时的启动命令 args: [string] # List, Not Required, 容器的启动命令参数列表 workingDir: string # String, Not Required, 容器的工作目录 volumeMounts: # List, Not Required, 挂载到容器内部的存储卷配置 - name: string # string, Not Required, 引用Pod定义的共享存储卷的名称， 需要用volumes[]部分定义的共享存储卷名称 mountPath: string # string, Not Required, 存储卷在容器内Mount的绝对路径，应少于512字符 readOnly: boolean # Boolean, Not Required, 是否为字符模式， 默认为读写模式 ports: # List, Not Required, 容器需要暴露的端口列表 - name: string # String, Not Required, 端口的名称 containerPort: int # Int, Not Required, 容器需要监听的端口号 hostPort: int # Int, Not Required, 不建议配置！！！容器所在主机需要监听的端口号， 默认与contianer相同。 设置hostPort时，同一台宿主主机上无法启动该容器的第二份副本。 protocol: string # String, Not Required, 端口协议，支持TCP和UDP，默认为TCP env: # List, Not Required, 容器运行前需要设置的环境变量列表 - name: string # string, Not Required, 环境变量的名称 value: string # string, Not Required, 环境变量的值 - name: string # string, Not Required, 环境变量的名称 valueFrom: # Object, Not Required, 环境变量来自哪里 configMapKeyRef: # Object, Not Required, 来自configMap name: string # string, Not Required, ConfigMap的名称 key: string # string, Not Required, configMap的键 resources: # Object, Not Required, 资源限制和资源需求设置 limits: # Object, Not Required, 资源限制的设置 cpu: string # string, Not Required, CPU的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string # string, Not Required, 内存的限制，单位可以为MiB/GiB等，将用于docker run --memory参数 requests: # Object, Not Required, 资源需求的设置 cpu: string # string, Not Required, CPU需求，单位为core数，容器启动的初始可用数量 memory: string # string, Not Required, 内存需求，单位可以为MiB,Gib等，容器启动时的初始可用数量 livenessProbe: # Object, Not Required, 对Pod内各个容器进行健康检查的设置，当探测无响应几次之后，系统会自动重启该容器。对一个容器仅需设置一种健康检查的方法。 exec: # Object, Not Required, 设置健康检查方式为exec方式 command: [string] # string, Not Required, 健康检查需要执行的命令或者脚本 httpGet: # Object, Not Required, 设置健康检查的方式为httpGet方式 path: string # string, Not Required, port: number # number, Not Required, host: string # string, Not Required, scheme: string # string, Not Required, httpHeaders: # Object, Not Required, - name: string # string, Not Required, value: string # string, Not Required, tcpSocket: # Object, Not Required, 设置健康检查方式为tcpSocket port: number # number, Not Required, initialDelaySeconds: 0 # number, Not Required, 容器启动完成后进行首次健康检查的时间，单位为秒 timeoutSeconds: 0 # number, Not Required, 健康检查等待响应超时时间，单位为秒，默认为1秒，超时则重启 periodSeconds: 0 # number, Not Required, 对容器健康检查的定期探测时间设置，单位为秒，默认为10秒探测1次 successThreshold: 0 # number, Not Required, failureThreshold: 0 # number, Not Required, securityContext: # 详见： https://www.kubernetes.org.cn/security-context-psp privileged: falserestartPolicy: [Always | Never | OnFailure]nodeSelector: object # Object, Not Required, 设置NodeSelector表示将该pod调度到包含这些label的Node上，以key: value形式指定imagePullSecrets: # Object, Not Required, pull镜像时使用的secret名称，以name: secretKey格式指定 - name: stringhostNetwork: false # Boolean, Not Required, 不建议配置！！！是否使用主机网络。默认为false，如果设置为true，不再使用docker网桥，该pod将无法再同一台宿主机上启动第二个副本volumes: # List, Not Required, 在该Pod定义的共享存储卷列表 - name: string # string, Not Required, 共享存储卷的名称。容器定义部分的container[].volumeMount[].name将引用此名称 emptyDir: &#123;&#125; # Object, Not Required, 类型为emptyDir的存储卷，表示与Pod生命周期相同的一个临时目录，其值为一个空对象 hostPath: # Object, Not Required, 类型为hostPath的存储卷，表示与挂载Pod所在宿主机的目录 path: string # string, Not Required, Pod所在主机的目录，将被用于容器中mount的目录 secret: # Object, Not Required, 类型为secret的存储卷，表示挂载集群预定义的secret对象到容器内部 secretName: string items: - key: string path: string configMap: # Object, Not Required, 类型为secret的存储卷， 表示挂载集群预定义的configMap对象到容器内部 name: string items: - key: string path: string]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记（6）- Dockerfile]]></title>
    <url>%2F2018%2F03%2F29%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89-Dockerfile_new%2F</url>
    <content type="text"><![CDATA[使用Dockerfile创建镜像Dockerfile是一个文本格式的配置文件，用户可以使用Dockerfile来快速创建自定义的镜像。 本章首先介绍Dockerfile典型的基本结构和它支持的众多指定，并具体讲解通过这些指令来编写定制镜像的Dockerfile，以及如何生成镜像。最后介绍使用Dockerfile的一些最佳时间经验。 基本结构Dockerfile有一行行命令语句组成，并且支持以#开头的注释行。 一般而言，Dockerfile分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令。 指令说明指令的一般格式为INSTRUCTION arguments，指令包括FROM/MAINTAINER/RUN等，参见下图： 下面本别介绍 1.FROM指定所创建镜像的基础镜像，如果本地不存在，则默认回去Docker Hub下载指定镜像。 格式为：FROM&lt;image&gt;，或FROM&lt;image&gt;:&lt;tag&gt;，或FROM&lt;image&gt;@&lt;digest&gt;。 任何Dockerfile中的第一条指令必须为FROM指令。并且，如果在同一个Dockerfile中创建多个镜像，可以使用多个FROM指令（每个镜像一次）。 2.MAINTAINER指定维护者信息，格式为MAINTAINER&lt;name&gt;。例如 1MAINTAINER image_creator@docker.com 该信息会写入生成镜像的Author属性域中。 3.RUN运行指定命令。 格式为:RUN&lt;command&gt;或RUN [&quot;exeutable&quot;,&quot;param1&quot;,&quot;param2&quot;]。 注意，后一个指令会被解析为json数组，因此必须用双引号。 前者默认将在shell终端中运行命令，即/bin/sh -c；后者则使用exec执行，不会启用shell环境。 指定其他终端类型可以通过第二种方式实现，例如RUN[&quot;bin/bash&quot;,&quot;-c&quot;,&quot;echo hello&quot;]。 每条RUN指令将在当前镜像的基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\来换行，例如： 12345RUN apt-get update \ &amp;&amp; apt-get install -y libsnappy-dev zlib1g-dev libbz2-dev \ &amp;&amp; rm -rf /var/cache/apt 4.CMDCMD指令用来指定启动容器时默认执行的命令。它支持三种格式： CMD[&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]使用exec执行，是推荐使用的方式； CMD command param1 param2在/bin/sh中执行，提交给需要交互的应用； CMD [&quot;param1&quot;,&quot;param2&quot;]提供给ENTRYPOINT的默认参数。 每个Dockerfile只能有一条CMD命令。如果指定了多条，只有最后一条会被执行。 如果用户启动容器时手动指定了运行的命令（作为run的参数），则会覆盖掉CMD指定的命令。 5.LABELLABEL指令用来指定生成镜像的元数据标签信息。 格式为LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt;….。例如 12345LABEL version=&quot;1.0&quot;LABEL description=&quot;this text ilustrates \ that label-values can span multiple lines.&quot; 6.EXPOSE声明镜像内服务所监听的端口。 格式为EXPOSE &lt;port&gt;[&lt;port&gt;…]。例如 1EXPOSE 22 80 8443 注意，该指令只是起到声明作用，并不会自动完成端口映射。 在启动容器时需要使用-P，Docker主机会自动分配一个宿主机的临时端口转发到指定的端口；使用-p可以具体指定那个宿主机的本地端口会映射过来。 7.ENV指定环境变量，在镜像生成过程中会被后续RUN指令使用，在镜像启动的容器中也会存在。 格式为ENV &lt;key&gt;&lt;value&gt;或&lt;key&gt;=&lt;value&gt;…。例如： 1234567ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgres &amp;&amp; ...ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH 指令指定的环境变量在运行时可以被覆盖掉，如docker run --env &lt;key&gt;=&lt;value&gt; built_image。 8.ADD该命令将复制指定的&lt;src&gt;路径下的内容到容器中的&lt;dest&gt;路径下。 格式为ADD&lt;src&gt;&lt;dest&gt;。 其中&lt;src&gt;可以是Dockerfile所在目录的一个相对路径（文件或目录），也可以是一个URL，还可以是一个tar文件（如果为tar文件，会自动解压到&lt;dest&gt;路径下）。&lt;dest&gt;可以是镜像内的绝对路径，或者相对于工作目录（WORKDIR）的相对路径。 路径支持正则格式，例如： 1ADD *.c /code/ 9.COPY格式为COPY &lt;src&gt; &lt;dest&gt; 复制本地主机的&lt;src&gt;（为Dockerfile所在目录的相对路径、文件或目录）下的内容到镜像中的&lt;dest&gt;下。目标路径不存在时，会自动创建。 路径同样支持正则格式。 当使用本地目录为源目录时，推荐使用COPY。 10.ENTRYPOINT指定镜像的默认入口命令，该入口命令会在启动容器时作为根命令执行，所有传入值作为该命令的参数。 支持两种格式： 123ENTRYPOINT [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]（exec调用执行）ENTRYPOINT command param1 param2 (shell中执行） 11.VOLUME创建一个数据卷挂载点。 格式为VOLUME[&quot;/data&quot;]。 可以从本地主机或其他容器挂载数据卷，一般用来存放数据库和需要保存的数据。 12.USER指定运行容器的用户名或UID，后续的RUN等指令也会使用指定的用户身份。 格式为UESR daemon。 当服务不需要管理员权限时，可以通过该命令指定运行用户，并且可以在之前创建所需要的用户。例如： 1RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 要临时获取管理员权限可以使用gosu或sudo。 13.WORKDIR为后续的RUN、CMD和ENTRYPOINT指令配置工作目录。 格式为WORKDIR/path/to/workdir。 可以使用多个WORKDIR指令，后续命令如果参数是相对路径，则会基于之前命令指定路径。例如： 1234567WORKDIR /aWORKDIR bWORKDIR cRUN pwd 则最终路径为/a/b/c。 14.AGR指定一些镜像内使用的参数（例如版本号信息等），这些参数在实行docker build命令时才以 --build-arg &lt;varname&gt;=&lt;value&gt;格式传入。 格式为ARG &lt;name&gt;[=&lt;default value&gt;]。 则可以用docker build --build-arg &lt;name&gt;=&lt;value&gt;来指定参数的值。 15.ONBUILD配置当所创建的镜像作为其他镜像的进出镜像时，所执行的创建操作指令。 格式为ONBUILD [INSTRUCTION]。 例如，Dockerfile使用如下的内容创建了镜像image-A： 1234567[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...] 如果基于image-A创建镜像时，新的Dockerfile中使用FROM image-a指定基础镜像，会自动执行ONBUILD指令的内容，等价于在后面添加两条指令： 1234567FROM image-a\#Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src 使用ONBUILD指令的镜像，推荐在标签中注明，例如ruby:1.9-onbuild 16.STOPSIGNAL指定所创建镜像启动的容器接收退出的信号值。例如： 1STOPSIGNAL signal 17.HEALTHCHECK配置所有启动容器如何进行健康检查（如何判断健康与否），自Docker 1.12开始支持。 格式有两种： HEALTHCHECK [OPTION] CMD command:很具所执行命令返回值是否为0判断； HEALTHCHECK NONE:禁止基础镜像中的健康检查。 OPTION支持： --interval=DURATION（默认为：30s）：过多久检查一次 --timeout=DURATION（默认为：30s）：每次检查等待结果的超市； --retries=N（默认为：30s）：如果失败了，重试几次才最终确定失败。 18.SHELL指定其他命令使用shell时的默认shell类型。 默认值为[&quot;/bin/bash&quot;,&quot;-c&quot;] 创建镜像 编写完成Dockerfile之后，可以通过docker build命令来创建镜像。 基本的格式为docker build [选项]内容路径，该命令读取指定路径下（包括子目录）的Dockerfile，并将该路径下的所有内容发送给Docker服务端，由服务端来创建镜像。因此除非生成镜像需要，否则一般建议放置Dockerfile的目录为空目录。有两点经验： 如果使用非当前路径下的Dockerfile，可以通过-f选项来指定其路径。 要指定生成镜像的标签信息，可以使用-f选项。 例如，指定Dockerfile所在路径为/tmp/docker_builder/，并且希望生成镜像标签为build_repo/first_image，可以使用下面的命令： $ docker build -t build_repo/fisrt_image /tmp/docker_builder/ 使用.dockerignore文件 可以通过.dockerignore文件（每一行添加一个匹配模式）来让Docker忽略匹配模式路径下的目录和文件。例如： #comment /temp //temp* temp? ~* 一些经验 精简镜像用途：尽量让每个镜像的用途都比较集中、单一，避免构造大而复杂、多功能的镜像； 选用合适的基础镜像：过大的基础镜像会造成臃肿的镜像，一般推荐为小巧的debian镜像； 提供足够清晰的命令注释和维护者信息：Dockerfile也是一种代码，需要考虑方便后续扩展和他人使用； 正确使用版本号：使用明确的版本号信息，如1.0，2.0，而非latest，将避免内容不一致可能引发的惨案； 减少镜像层数：如果希望所生成镜像的层数尽量少，则要尽量合并指令，例如多个RUN指令可以合并为一条； 及时删除临时文件和缓存文件：特别是在执行apt-get指令后，/var/cache/apt下面会缓存一些安装包； 提高生成速率：合理使用缓存，减少内容目录下的文件，或使用.dockerignore文件指定等。 调整合理的指令顺序：在开启缓存的情况下，内容不变的指令尽量放在前面，这样可以尽量复用； 减少外部源的干扰：如果确实要从外部引入数据，需要指定特定持久的地址，并带有版本信息，让他人可以重复而不出错。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记（5）- 端口映射与容器互联]]></title>
    <url>%2F2018%2F03%2F29%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89-%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BA%92%E8%81%94_new%2F</url>
    <content type="text"><![CDATA[端口映射与容器互联除了通过网络访问外，Docker还提供了两个很方便的功能来满足服务访问的基本需求：一个是允许映射容器内应用的服务端口到本地宿主主机；另一个是互联机制实现多个容器间通过容器名来快速访问。 端口映射实现访问容器从外部访问容器应用在启动容器的时候，如果不指定对应的参数，在容器外部是无法通过网络来访问容器内的网络和服务的。 当容器中运行一些网络应用，要让外部访问这些应用时，可以通过-P或-p参数来指定端口映射。 当使用-P标记时，Docker会随机映射一个49000~49900的端口到内部容器开放的网络端口。 映射所有接口地址hostPort:containerPort：将本地的5000端口映射到容器的5000端口 此时会绑定本地所有接口上的所有地址，多次使用-p标记可以绑定多个端口。 映射到指定地址的端口ip::containerPort：映射到指定地址的任意的端口： 使用 ip::containerPort 绑定 localhost 的任意端口到容器的 5000 端口，本地主机会自动分配一个端口。 映射到指定地址的端口当使用-p 标记时 则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。支持的格式有 ： ip:hostPort:containerPort ：指定映射使用一个特定地址，比如localhost地址127.0.0.1: 查看映射端口配置使用docker port命令来查看当前映射的端口配置，可以查看到绑定的地址。 $ docker port &lt;container_id | container_name&gt; 注意：容易有自己的内部网络和IP地址，使用docker inspect + 容器Id可以获取容器的具体信息。 互联机制实现便捷访问容器的互联（linking)是一种让多个容器中应用进行快速交互的方式。他会在源和接收容器之间创建连接关系，接收容器可以通过容器名快速访问到容器源，而不用指定具体的IP地址。 自定义容器命名连接系统依据容器的名称来执行。因此需要定义一个好记的容器的名字。 虽然当创建容器的时候，系统会默认分配一个名字，单自定义容器名字有两个好处： 自定义命名比较好记，比如一个Web容器，我们可以起名为web，一目了然； 当要连接其他容器时，即便重启，也可以使用容器名而不用改变，比如web容器连接到db容器。 使用–name标记可以为容器自定义命名： 使用docker inspect命名查看 注意： 容器的名称是唯一的。如果已经命名了一个叫web的容器，当你要再次使用web这个名称的时候，需要先用docker rm来删除之前创建的同名容器。 在执行docker run的时候，如果添加–rm标记，则容器在终止后会立刻删除。注意，–rm和-d参数不能同时使用。 容器互联使用–link参数可以让容器之间安全地进行交互。 下面先创建一个新的数据库容器 查看创建的容器信息 然后创建一个新的web容器，并将它连接到db容器 此时，db容器和web容器建立互联关系 –link参数的格式为–link name:alias，其中name是要连接的容器名称，alias是这个连接的别名。 使用docekr ps来查看容器的连接，如下图所示： Docker相当于在两个互联的容器之间创建了一个虚机通道，而且不用映射他们的端口到宿主主机上。在启动db容器的时候并没有使用-p和-P标记，从而避免了暴露数据库服务端口到外部网络上。 Docker通过两种方式为为容器公开连接信息： 更新环境变量； 更新/etc/hosts文件。 使用env命名来查看web容器环境变量： 其中DB_开头大写的环境变量是供web容器连接db容器使用的，前缀采用大写的连接别名。 除了环境变量之外，Docker还添加host信息到父容器的/etc/hosts文件。下面是父容器web的hosts文件： 这里有两个hosts信息，第一个是web容器，web容器用自己的id作为默认主机名，第二个是容器的ip和容器的别名以及id。 可以在web容器中安装Ping命令来测试与db容器的连通： 用Ping来测试db容器，他会解析成172.17.0.5。用户可以连接多个子容器到父容器，比如可以连接多个web到同一个db容器上。 本文小结毫无疑问，容器服务的访问是很关键的一个用途。本文通过具体案例讲解了Docker容器服务访问的两大基本操作，包括基础的容器端口映射机制和容器的互联机制。同时，Docker目前可以成熟的支持Linux系统自带的网络服务和功能，这既可以利用现有成熟的技术提供稳定支持，又可以实现快速的高性能转发。 在生产环境中，网络方面的需求更加复杂多变，包括跨主机甚至夸数据中心的通信，这时候往往就需要引入额外的机制，例如SDN或NFV（网络功能虚拟化）的相关技术。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记（4）- Docker数据卷]]></title>
    <url>%2F2018%2F03%2F29%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89-Docker%E6%95%B0%E6%8D%AE%E5%8D%B7_new%2F</url>
    <content type="text"><![CDATA[Docker数据卷生产环境中使用Docker的过程中，往往需要对数据进行持久化，或者需要在多个容器之间进行数据共享，这必然涉及容器的数据管理操作。 容器中管理数据主要有两种方式： 数据卷(Data Volume)：容器内数据直接映射到本地主机环境； 数据卷容器(Data Volume Containers)：使用特定容器维护数据卷。 本文将首先介绍如何在容器内创健数据卷，并且把本地的目录或文件挂载到容器内的数据卷中。接下来会介绍如何使用数据卷容器在容器和主机、容器和容器之间共享数据，并实现数据的备份和恢复。 数据卷数据卷是一个可供容器使用的特殊目录，它将主机操作系统目录直接映射进容器，类似于Linux中的mout操作。 数据卷可以提供很多有用的特性，如下所示： 数据卷可以在容器之间共享和重用，容器间传递数据将变得高效方便； 对数据卷内数据的修改会立马生效，无论是容器内操作还是本地操作； 对数据卷内数据的修改会立马生效，无论是容器内操作还是本地操作； 对数据卷的更新不会影响到镜像，解耦了应用和数据； 卷会一直存在，只有没有容器使用，可以安全地卸载它。 在容器中创建一个数据卷在用docker run命令的时候，使用-v参数可以在容器内创建一个数据卷。多次重复使用-v标记可以创建多个数据卷。 使用图中的命令在镜像ubuntu:latest上创建一个在后台运行名为volume_test的容器，并且创建了一个名为/data_volume的数据卷。 进入容器查看，已经有了data_volume目录 挂载一个本地目录作为数据卷下面的命令创建了一个容器，并且将容器中的数据卷/data_valume挂载到本地/Users/xiangqilin/docker/data_volume。 进入容器，在数据卷/data_volume中创建一个文件test，然后在本地/Users/xiangqilin/docker/data_volume查看，容器中数据卷的数据已经映射到了本地。 挂载数据卷的默认权限为读写（rw）,可以将其改为只读（ro)。 只需要在加上ro参数，如下所示： 挂载一个本地文件作为数据卷（不推荐）用下面的命令可以挂载一个文件到本地，记录容器中执行的bash命令。 注意： 如果直接挂载一个文件倒容器，使用文件编辑工具，包括vi或者sed –in-place的时候，可能会造成文件inode的改变，从Docker 1.1.0起，这会导致报错信息。所以推荐的方式是直接挂载该文件所在的目录。 数据卷容器如果用户需要在多个容器之间共享一些持续更新的数据，最简单的方式是使用数据卷容器。数据卷容器也是一个容器，但是它的目的是专门用来提供数据卷供其他容器挂载的。 首先创建一个数据卷容器volume_container，并在其中创建一个/db_data 创建一个容器db1，挂载volume_container中的数据卷/db_data,使用ls可以看到根目录下已经有了db_data 创建一个容器db2，挂载volume_container中的数据卷/db_data，使用ls可以看到根目录下已经有了db_data 下面的代码将测试数据卷容器挂载的目录是否在各个容器之间是同步的： 第一步在数据卷容器的数据卷中创建一个test文件。 第二步在db1容器中，查看其挂载的数据卷，是否有test文件。 可以看到容器db1的挂载的数据卷中已经有了test文件，说明被挂在的数据卷容器中的数据卷和挂载数据卷是同步的。 进一步验证，我们将在容器db1挂载的数据卷中创建一个文件，然后验证db2中是否也存在： 可以看出，在容器db1中的挂载数据卷中创建的test2文件，已经同步到了db2的挂载数据卷中，所以只要是挂载的同一数据卷容器的数据卷，那么他们之间都是同步的。 可以多次使用--volumes-from参数来从多个容器挂载多个数据卷。还可以从其他已经挂载了容器卷的容器来挂载数据卷。 注意： 使用--volume-from参数所挂载数据卷的容器自身并不需要保持在运行状态。 如果删除了挂载的容器（volume_container、db1和db2），数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载着他的容器时显示使用docker rm -v命令来指定同时删除关联的容器。 利用数据卷容器来迁移数据可以利用数据卷容器对其中的数据卷进行备份、恢复，以实现数据的迁移。下面介绍这两个操作。 备份使用下面的数据来备份数据卷db_data 这个命令有点复杂，分析一下：首先创建一个ubuntu镜像的容器，然后挂载数据卷容器volume_container的数据卷db_data。然后把自身容器中/backup目录挂载到本地/Users/xiangqlin/docker/backup。容器启动后，执行tar -cvf /backup/backup.tar /db_data将挂载的数据卷/db_data压缩到/backup/backup.tar。因为/backup目录被挂载到了本地，所以在本地也备份了数据卷的数据。 恢复]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记（3）- Docker仓库]]></title>
    <url>%2F2018%2F03%2F29%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89-Docker%E4%BB%93%E5%BA%93_new%2F</url>
    <content type="text"><![CDATA[访问Docker仓库仓库（Repository）是集中存放镜像的地方，分公共仓库和私有仓库。一个容器混淆的概念是注册服务器（Registry)。实际上注册服务器是存放仓库的具体服务器，一个服务器上可以有多个仓库，而每个仓库下面可以有多个镜像。从这方面来说，可以将仓库看做一个具体的项目和目录。例如对于仓库地址private-docker.com/ubuntu来说，private-docker.com是注册服务器地址，ubuntu是仓库名。 Docker Hub公共镜像市场目前Docker官方维护了一个公共镜像仓库http://hub.docker.com，其中包括超过15000个镜像。大部分镜像需求，都可以通过在Docker Hub中直接下载镜像来实现。 登录docker login命令来输入用户名、密码登录。 基本操作用户无需登录即可通过docker search命令来查找官方仓库中的镜像，并利用docker pull命令来下载到本地。 根据是否为官方提供，可将这些镜像资源分为两类。一种是类似centos这样的基础镜像，成为基础或根镜像。这些镜像是由Docker公司创建、验证、支持、提供。这样的镜像往往使用单个单词名字。 还有一种类型，比如ansible/centos7-ansible镜像，它是由用户ansible创建并维护的，带有用户名为前缀，表明是用户下的某仓库。可以通过用户名称前缀user_name/镜像名称来指定使用某个用户提供的镜像。 另外在查找的时候，通过-s N参数可以指定显示评价为N星以上的镜像。 下载centos镜像到本地，如下所示： 用户也可以通过docker push命令来将本地镜像推送到Docker Hub。 提示：Ansible是知名自动化部署配置管理工具。 自动创建自动创建（Automated Builds）功能对于需要经常升级镜像内程序来说，十分方便。有时候，用户创建了镜像，安装了某个软件，如果软件版本发布新版本则需要手动跟新。 而自动创建允许用户通过Docker Hub指定跟踪一个目标网站（目前支持GitHub或BitBucket)上的项目，一旦项目发生新的提交，则自动执行创建。 要配置自动创建，包括如下的步骤： 创建并登陆Docker Hub，以及目标网站；并在目标网站中链接账户到Docker Hub； 在Docker Hub中配置一个“自动创建”； 选取一个目标网站中的项目（需要含Dockerfile）和分支； 指定Dockerfile位置，并且提交创建 搭建本地仓库使用registry镜像创建私有仓库安装docker后，可以通过官方提供的registry镜像来简单搭建一套本地私有仓库环境： $docker run -d -p 5000:5000 regitry 这将自动下载并启动一个registry容器，创建本地的私有仓库服务。 默认情况下，会将仓库创建在容器的/tmp/registry目录下。可以通过-v参数来将镜像文件夹存放在本地的指定路径，例如下面的例子将镜像放到/Users/xiangqilin/registry 注意registry:2是ash shell，如果执行$docker exec -it nifty_mahavira /bin/bash会报错 向搭建好的本地仓库push成功之后，-v参数指定的映射目录也会存储上传的镜像]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记（2）- Docker容器]]></title>
    <url>%2F2018%2F03%2F29%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89-Docker%E5%AE%B9%E5%99%A8_new%2F</url>
    <content type="text"><![CDATA[docker容器容器是Docker一个核心概念。简单来说，容器是镜像的一个运行示例。所不同的是，镜像是静态的只读文件，而容器带有运行时需要的可写层文件。如果认为虚拟机是模拟运行的一整套操作系统（包括内核、应用运行态环境和其他系统环境）和跑在上面的应用，那么Docker容器就是独立运行的一个（或一组）应用，以及他们必要的运行环境。 创建容器新建容器可以使用docer create命令新建一个容器，例如： 使用docker create命令新建的容器处于停止状态，可以使用docker start命令来启动它。 create命令和run命令支持的选项都十分复杂，主要包括如下几大类： 与容器运行模式相关 与容器和环境配置相关 与容器资源限制和安全保护相关 表1 create命令与容器运行模式相关的选项 表2 create命令与容器环境和配置相关的选项 表3 create命令与容器资源限制和安全保护相关的选项 其他还比较重要的选项： -l, --label=[]：以键值对方式指定容器的标签信息 --label-file=[]：从文件中读取标签信息 启动容器使用docker start命令来启动一个已经创建的容器，例如启动刚才创建的容器 可以看出 $docker start aae命令启动了刚才状态为created的容器，参数aae是该容器id的前三个字符，说明只要可以唯一区别一个容器，id可以简写。 新建并启动容器除了创建容器后通过start命令来启动，可以直接新建并启动容器。所需要的命令主要为docker run，等价于docker create命令，在执行docker start命令。 用下面的命令启动一个容器，并在容器中打印“docker run test” 查看容器，可以发现，执行完命令之后，就退出了，状态变为exited 当利用docker run来创建并启动容器时，Docker在后台运行的标准操作标准包括： 检查本地是否存在指定的镜像，不存在就从共有仓库下载 利用镜像创建一个容器，并启动该容器 分配一个文件系统给容器，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中 从网桥的地址池配置一个IP地址给容器 执行用户指定的应用程序 执行完毕后容器被自动终止 下面的命令创建并运行了一个容器，还启动了一个bash终端，允许和用户交互 其中-t选项让Docker分配了一个伪终端（pseudo-tty）并绑定到容器的标准输入上，-i则让容器的标准输入保持打开。下面用ps命令来查看进程，只有bash和ps命令，没有其他进程。最后用exit命令退出或者ctrl+d 某些时候，执行docker run会出错，因为命令无法正常执行容器会直接退出，此时可以查看退出的错误代码。 默认情况下，常见错误代码包括： 125：Docker daemon执行出错，例如指定不支持的Docker命令参数； 126：所指定的命令无法执行，例如权限出错。 127：容器内命令无法找到 命令执行出错后，会默认返回错误码。 守护态运行守护进程：守护进程是一个在后台运行并且不受任何终端控制的进程。Unix操作系统有很多典型的守护进程(其数目根据需要或20—50不等)，它们在后台运行，执行不同的管理任务。 —-百度 更多的时候，需要让Docker容器在后台以守护态（Daemonized）形式运行。此时，可以通过添加-d参数来实现。 例如下面的命令会在后台容器运行： 此时，要获取容器的输出信息，可以如下使用docker logs命令： 终止容器可以使用docker stop来终止一个运行中的容器。该命令格式为 $docker stop [-t | --time[=10]][CONTAINER…] 首先向容器发送SIGTERM信号，等待一段超时时间（默认10秒）后，再发送SIGKILL信号来终止容器 注意： docker kill命令会直接发送SIGKILL信号来强行终止容器。 此外，当Docker容器中指定的应用终结时，容器也会自动终止。例如对于上一节中只启动了一个终端的容器，用户通过exit命令或Ctrl+D来退出终端时，所创建的容器立刻终止，处于exited状态。 可以通过start命令来使终止态的容器重新启动 可以使用restart命令先终止容器，再重新启动。 进入容器在使用-d参数时，容器启动后会进入后台，用户无法看到容器中的信息，也无法进行操作。 这个时候如果需要进入容器进行操作，有多种方法，包括官方的attach或exec命令，以及第三方的nsenter工具等。 attach命令attacn是docker自带的命令，命令格式为： docker attach [--detach-keys[=[]]][--no-stdin] [--sig-proxy[=true]] CONTAINER 支持三个主要选项： --detach-keys=[=[]]：指定退出attach模式的快捷键序列，默认为CTRL-P,CTRL-Q; --no-sdtin=true|false：是否关闭标准输出，默认是保持打开的； --sig-proxy=true|false：是否代理收到的系统信号给应用进程，默认为true。 但是有时候使用attach命令并不方便。当多个窗口同时用attach命令连到同一个容器时，多有窗口都会同步显示。当某个窗口因为命令阻塞时，其他窗口也无法执行操作了。 exec命令Docker从1.3.0.版本起提供了一个更加方便的exec命令，可以在容器内直接执行任意命令。 docker exec [-d|--detach][--detach-key[=[]]] [-i|--interactive][—privileged] [-t|--tty][-u|--user[=USER]] CONTAINER COMMAND [ARG…] 比较重要的参数： -i, --interactive=true|false：打开标准输入接受用户输入命令，默认为false； --privileged=true|false：是否给执行命令以高权限，默认为false； -t, --tty=true|false：分配伪终端，默认为false。 -u, --user=&quot;&quot;：执行命令的用户或id 例如进入刚才的容器，并启动一个bash： 从上面可以看出，只能在一个运行的容器才能使用exec命令； 先启动容器，在执行exec命令： 可以看到，一个bash终端打开了，在不影响容器内其他应用的前提下，用户可以很容易与容器进行交互。 注意：通过指定-it参数来保持标准输入打开，并且分配一个伪终端。通过exec命令对容器执行操作是最为推荐的方式。 nsenter工具 删除容器可以使用docker rm命令来删除处于终止或退出状态的容器，命令格式为 docker rm [-f|--force][-l|--link] [-v|--volumes] CONTAINER[CONTAINER…] 主要支持的选项包括： -f, --force=false：是否强制终止并删除一个运行中的容器； -l, --link=false：删除容器的链接，但保留容器； -v, --volumes=false：删除容器挂载的数据卷。 例如，查看处于终止状态的容器，并删除： 默认情况下，docker rm命令只能删除处于终止或退出状态的容器，并不能删除还处于运行状态的容器。 如果直接删除一个运行中的容器，可以添加-f参数。Docker会先发送SIGKILL信号给容器，终止其中的应用，之后强行删除，如下所示： 导入和导出容器某些时候，需要将容器从一个系统迁移到另外一个系统，此时可以使用Docker的导入和导出的功能。这也是Docker自身提供的一个重要的特性。 导出容器导出容器是指导出一个已经创建的容器到一个文件，不管此时这个容器是否处于运行状态，可以使用 $docker export命令，该命令的格式为： $docker export [-o|--output[=&quot;&quot;]]CONTAINER。其中，可以通过-o选项来指定导出的tar文件名，也可以通过重定向来实现。 下面分别使用-o参数和重定向的方式导出容器： 时候可以将导出的tar文件输出到其他的机器上，然后再通过导入命令导入到系统中，从而实现容器的迁移。 导入容器导出的文件又可以使用docker import命令导入变成镜像，该命令格式为： docker import [-c|--change[=]]][-m|--message[=MESSAGE]] file|URL|-[REPOSITORY[:TAG]] 可以通过-c, --change=[]选项在导入的同时执行对容器进行修改的Dockerfile指令。 也可以使用docker load命令来导入镜像存储文件到本地镜像库，也可以使用docker import命令来导入一个容器快照到本地镜像库。 这两者的区别在于容器快照文件将丢失所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也更大。此外，从容器快照导入时可以重新定义标签等元数据。 exported-imported和saved-loaded的区别 导出后再导入(exported-imported)的镜像会丢失所有的历史，而保存后再加载（saveed-loaded）的镜像没有丢失历史和层(layer)。这意味着使用导出后再导入的方式，你将无法回滚到之前的层(layer)，同时，使用保存后再加载的方式持久化整个镜像，就可以做到层回滚（可以执行docker tag 来回滚之前的层）。–百度 小结 容器是直接提供应用服务的组件，也是docker实现快速启停和高效服务性能的基础。 命令小结create start restart run stop attach exec rm export -————- 导入和导入镜像一样： import–export save—-load 再生产环境中，因为容器自身的轻量级特性，推荐使用容器时在一组容器前引入HA(High Availability，高可靠性）机制。例如使用HAProxy工具来代理容器访问，这样在容器出现故障时，可以快速切换到功能正常的容器。此外，建议通过指定合适的容器重启策略，来自动重启退出的容器。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记（1）- Docker镜像]]></title>
    <url>%2F2018%2F03%2F29%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89-Docker%E9%95%9C%E5%83%8F_new%2F</url>
    <content type="text"><![CDATA[获取镜像$docker pull NAME[:TAG] 如果不加TAG则表示下载最新的镜像，一个镜像由“名称”+“标签决定” 使用不同镜像仓库服务器情况下，可能会出现镜像重名： 严格的将，镜像的仓库名称中还应该添加仓库地址（即registry，注册服务器）作为前缀，如果使用Docker Hub服务，该前缀可以忽略。 即：$docker pull ubuntu:14.04相当于$docker pull registry.hub.docker.com/ubuntu:14.04 pull子命令： -a, --all-tags=true|false：是否获取仓库中的所有镜像，默认为否。 使用镜像：例如利用该镜像创建一个容器，在其中运行bash应用，执行ping localhost: $docker run -it unbuntu[:TAG] bash root@f46310567509:/# ping localhost 如果提示command not found，则 $apt-get update $apt-get install iputils-ping 详细见：http://blog.csdn.net/silentwolfyh/article/details/52336007 查看镜像信息docker images [OPTIONS][REPOSITORY[:TAG]] OPTIONS说明： -a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； —digests :显示镜像的摘要信息； -f :显示满足条件的镜像； —format :指定返回值的模板文件； --no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 使用docker images命令列出镜像 镜像的仓库：ubuntu的系列镜像由ubuntu仓库保存 标签 镜像ID，镜像的唯一标识 创建的时间，说明镜像最后跟新时间 镜像大小 镜像标签使用tag命令添加镜像标签，可以发现多了一个myubuntu:latest标签，其id和ubuntu:latest是一样的，其实他们都指向同一个镜像，只是别名不同，标签相当于起到了链接的作用。 查看镜像详细信息使用docker inspect命令查看详细信息，包括制作者、适应架构、各层的数字摘要等： 输出一个json文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394[ &#123; "Id":"sha256:8b72bba4485f1004e8378bc6bc42775f8d4fb851c750c6c0329d3770b3a09086", "RepoTags":[ "myubuntu:latest", "ubuntu:latest" ], "RepoDigests":[ "ubuntu@sha256:2b9285d3e340ae9d4297f83fed6a9563493945935fc787e98cc32a69f5687641" ], "Parent":"", "Comment":"", "Created":"2017-09-13T03:58:50.383839319Z", "Container":"ee87d884293ece0d9fa040a43ffb75097264185f94437a0d1fc2ddfd3c82ca4b", "ContainerConfig":&#123; "Hostname":"ee87d884293e", "Domainname":"", "User":"", "AttachStdin":false, "AttachStdout":false, "AttachStderr":false, "Tty":false, "OpenStdin":false, "StdinOnce":false, "Env":[ "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" ], "Cmd":[ "/bin/sh", "-c", "#(nop) ", "CMD ["/bin/bash"]" ], "ArgsEscaped":true, "Image":"sha256:5bf9c8f025cb9bdfec431fbf2a39e1d25117a94ce2b10db01db9630addfc5e37", "Volumes":null, "WorkingDir":"", "Entrypoint":null, "OnBuild":null, "Labels":&#123; &#125; &#125;, "DockerVersion":"17.06.2-ce", "Author":"", "Config":&#123; "Hostname":"", "Domainname":"", "User":"", "AttachStdin":false, "AttachStdout":false, "AttachStderr":false, "Tty":false, "OpenStdin":false, "StdinOnce":false, "Env":[ "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" ], "Cmd":[ "/bin/bash" ], "ArgsEscaped":true, "Image":"sha256:5bf9c8f025cb9bdfec431fbf2a39e1d25117a94ce2b10db01db9630addfc5e37", "Volumes":null, "WorkingDir":"", "Entrypoint":null, "OnBuild":null, "Labels":null &#125;, "Architecture":"amd64", "Os":"linux", "Size":120102168, "VirtualSize":120102168, "GraphDriver":&#123; "Data":&#123; "LowerDir":"/var/lib/docker/overlay2/579119ec0ba94ba9c2b510b75228f366c14fc1a29302f29a7c915946f9038c54/diff:/var/lib/docker/overlay2/2286560d99b26f3e25b0cb65f100915b2bf3848ff483b2b46f036a782fe92e87/diff:/var/lib/docker/overlay2/90c4f96886f317dfc958dededefa77f9a648011b7002e42c458b61022e950ab9/diff:/var/lib/docker/overlay2/fba0d170b11c979215b1fe227f86b3499427f1908f3f70dbddde2f94d433ee77/diff", "MergedDir":"/var/lib/docker/overlay2/95395204f6fdd03d04da60ee7d1c1e7ff4bf7e96eb789eca0cdeedf58125aecb/merged", "UpperDir":"/var/lib/docker/overlay2/95395204f6fdd03d04da60ee7d1c1e7ff4bf7e96eb789eca0cdeedf58125aecb/diff", "WorkDir":"/var/lib/docker/overlay2/95395204f6fdd03d04da60ee7d1c1e7ff4bf7e96eb789eca0cdeedf58125aecb/work" &#125;, "Name":"overlay2" &#125;, "RootFS":&#123; "Type":"layers", "Layers":[ "sha256:8aa4fcad5eeb286fe9696898d988dc85503c6392d1a2bd9023911fb0d6d27081", "sha256:ebf3d6975c708f538b14a5267afd2c4c64e8243d195aa11d878e566a7e64c727", "sha256:a76db6d8fac422acd5fb6c28166c906c202639e4e833cf88c7d4965b806c5437", "sha256:cd1d6655b4e44bb95df75bd2ecde4ad6799dd23337a9dedadf6e0b7f0efdc27e", "sha256:3996d0debc49f9a96c25d4ab7a5c9e824229c09976551b80ab0da70fa993a10d" ] &#125; &#125;] 可以使用-f参数来指定获取某一项参数： 参考http://www.cnblogs.com/boshen-hzb/p/6376674.html 镜像历史使用history命令查看镜像历史。 镜像文件由多个层组成，可以用history命令，显示各层的创建信息。 搜寻镜像使用docker search命令可以搜索远端仓库中共享的镜像，默认搜索官方仓库总的镜像。用法为docker search TERM，支持的主要参数包括： --automated=true|false:仅显示自动创建的镜像，默认为否 --no-trunc=true|false:输出信息不截断显示，默认为否 -s, --stars=X：指定仅显示评价为指定星级以上的镜像，默认为0，即输出所有镜像。 例如，搜索所有自动创建的3星级以上的带 nginx关键字的镜像: 删除镜像使用docker rmi命令可以删除镜像，命令格式为docker rmi IMAGE [IMAGE]，其中IMAGE可以为标签或ID。 例如，要删除掉myubuntu:latest镜像，可以使用如下命令 可以看出，如果一个镜像有多个标签，只会删除多个标签中指定的标签而已，镜像不会受影响。 如果某个镜像只有一个标签时，则要注意，删除标签就会删除镜像。 现在只有一个标签，然后尝试删除镜像： 删除失败，因为有容器还在镜像上面运行 可以使用docker ps -a命令查看本机上的所有容器： 可以看出有容器在镜像上运行，所以不能直接删除，当然可以使用命令 $docker rmi -f ubuntu:latest来强制删除，但是不建议这样做，建议先删除容器，然后删除镜像： 我们先使用-f参数来删除httpd镜像： 但是发现镜像依然存在 再次使用$docker ps -a命令来查看容器，发现仍然有容器运行 所以并不推荐使用-f参数来强制删除，下面是正确的步骤，先删除运行在镜像上面的容器，然后再删除镜像。 现在先使用$docker rm CONTAINER_ID删除容器，有一个错误， 可以看出id为a420…的容器还在运行，所以不能删除，使用$docker ps -a查看删除情况， 状态为未运行的容器已经删除，然而运行状态的并没有删除 所以现在需要先使用$docker stop CONTAINER_ID停止运行的容器 可以看出id为a420…的容器状态已经改变，现在可以删除了 可以看出，容器已经被删除了，现在就可以去删除httpd镜像了： 使用$docker rmi IMAGE_ID删除镜像，然后使用$docker images查看删除情况，可以看出httpd镜像已经全部删除完了。 一次性删除所有容器 创建镜像创建镜像的方法主要有三种： 基于已有镜像的容器创建 基于本地模板导入 基于Dockerfile创建 这里主要介绍前两种方法，第三种方法后续学习笔记介绍。 基于已有镜像的容器创建主要使用docker commit命令。命令格式为 docker commit [OPTIONS] CONTAINER [REPOSITORY [:TAG]] ​ 所基于的容器 创建的镜像的标签 主要的选项包括： -a, author=&quot;&quot;：作者信息 -c, --change=[]：提交的时候执行Dockerfile命令，包括CMD|ENTRYPOINT|ENV|EXPOSE|LABEL|USER|VOLUME|WORKDIR等 -m, --message=&quot;&quot;：提交消息 -p, --pause=true：提交时暂停容器运行 下面演示怎么根据已有镜像的容器创建一个新的镜像。 首先启动一个镜像，然后进行进行一些操作，例如创建一个test文件，然后退出： $docker run -it ubuntu:latest /bin/bash root@3548583db1bb:/# touch test root@3548583db1bb:/# exit 记住容器的ID为3548583db1bb。 然后使用docker images命令查看刚才创建的镜像，也可以使用docker ps -a来查看容器。 接下来使用·docker commit·命令根据容器3548583db1bb来创建一个新的镜像。 提交信为：Added a new file 作者信息为：Docker Newbee 源容器：3548583db1bb 被创建的容器的标签：test:0.1 使用docker image命令查看刚才创建的test:0.1镜像 基于本地模板导入用户可以直接从一个操作系统模板文件导入一个镜像文件，主要使用docker import命令。命令格式为 docker import [OPTIONS] file |URL| -[REPOSITORY[:TAG]] OPTIONS说明： -c：应用docker指令创建镜像文件 -m：提交信息 要直接导入一个镜像，可以使用OpenVZ提供的模板来创建，或者用其他已导出的镜像模板来创建。OPENVZ模板的下载地址为http://openvz.org/Dowload/templates/precreated 存出和载入镜像用户可以使用docker save和docker load命令来存出和载入镜像。docker load一般只用于导入由docker save导出的镜像，导入后的镜像跟原镜像一模一样，拥有相同的镜像id和分层内容。docker import不能用于导入标准的Docker镜像，而是用于导入包含根文件系统的归档，并将之变成Docker镜像。 存出镜像docker save：将指定镜像保存成tar归档文件。 语法：docker save [OPTIONS] IMAGE [IMAGE…] OPTIONS说明： -o：输出到文件 得到了my_ubuntu.tar归档 载入镜像可以使用docker load将导出的tar文件再导入到本地文件镜像库，例如从文件my_ubuntu.tar docker load：从一个在stdin上的tar归档文件中装载镜像 OPTIONS说明： -i, --input=&quot;&quot; 从一个tar归档文件中读入，而不是从stdin中 $docker load --input my_ubuntu.tar 或 $docker load &lt; my_ubuntu.tar 上传镜像可以使用docker push命令上传镜像到仓库，默认上传到Docker Hub官方仓库（需要登录）。 命令格式为： docker push NAME[:TAG] | REGISTRY_HOST[:REGISTRY_PORT] /]NAME [:TAG] 先给需要push的镜像打上标签，因为的我用户名叫kylinxiang，所以改为kylinxiang/push_test:1.0 默认是Push到Docker Hub，所以可以看到最后push到的仓库为docker.io/kylinxiang 之后就可以在Docker Hub上面就可以查看Push的镜像]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊JPA Criteria查询中的坑]]></title>
    <url>%2F2018%2F03%2F16%2F%E8%81%8A%E8%81%8AJPA-Criteria%E6%9F%A5%E8%AF%A2%E4%B8%AD%E7%9A%84%E5%9D%91_new%2F</url>
    <content type="text"><![CDATA[聊聊JPA Criteria查询中的坑JPA Criteria查询被称作动态安全类型查询，比JPQL这种方式更加健壮。关于JPA Criteria查询在IBM社区有一篇很好的文章，这里我就不去Copy（尊重默默为社区奉献的同行兄弟），请移步https://www.ibm.com/developerworks/cn/java/j-typesafejpa/ Bug复现场景创建一个Student类，然后创建其StaticMetaModel Student_类；然后使用Critirial查询年龄大于20的Student。场景很简单，但是。。。结果很意外，让我们来看看相关配置和代码。 配置依赖配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.kylin&lt;/groupId&gt; &lt;artifactId&gt;jpa-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;jpa-demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8 &lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; yml配置文件12345678910111213141516server: port: 8800spring: datasource: password: 123456 username: root driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/jpaadvance jpa: database-platform: mysql hibernate: ddl-auto: update show-sql: true properties: hibernate: dialect: org.hibernate.dialect.MySQL5Dialect 代码Student类代码123456789101112131415161718192021package com.kylin.jpademo.domain;import org.hibernate.annotations.GenericGenerator;import javax.persistence.*;import java.io.Serializable;@Entity@Tablepublic class Student implements Serializable &#123; private static final long serialVersionUID = -7681363673194194734L; @Id @GeneratedValue(generator = "system-uuid") @GenericGenerator(name = "system-uuid", strategy = "uuid") @Column(updatable = false, nullable = false) private String id; private String name; private int age; //geter and seter Student_类代码12345678910111213package com.kylin.jpademo.domain;import com.kylin.jpademo.domain.metamodel.Student;//重点看这里import javax.persistence.metamodel.SingularAttribute;import javax.persistence.metamodel.StaticMetamodel;@StaticMetamodel(Student.class)public class Student_ &#123; public static volatile SingularAttribute&lt;Student, String&gt; id; public static volatile SingularAttribute&lt;Student, String&gt; name; public static volatile SingularAttribute&lt;Student, Integer&gt; age;&#125; Repository接口123456789package com.kylin.jpademo.repository;import com.kylin.jpademo.domain.metamodel.Student;import java.util.List;public interface StudentRepository &#123; List&lt;Student&gt; findStudentByAgeLessThan(int age);&#125; Repository实现1234567891011121314151617181920212223242526272829303132333435package com.kylin.jpademo.repository.impl;import com.kylin.jpademo.domain.metamodel.Student;import com.kylin.jpademo.domain.Student_;import com.kylin.jpademo.repository.StudentRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;import org.springframework.transaction.annotation.Transactional;import javax.persistence.EntityManager;import javax.persistence.TypedQuery;import javax.persistence.criteria.*;import java.util.List;@Transactional@Repositorypublic class StudentRepositoryImpl implements StudentRepository &#123; @Autowired EntityManager em; @Override public List&lt;Student&gt; findStudentByAgeLessThan(int age) &#123; System.out.println(age); CriteriaBuilder cb = em.getCriteriaBuilder(); CriteriaQuery&lt;Student&gt; cq = cb.createQuery(Student.class); Root&lt;Student&gt; s = cq.from(Student.class); // from ... Path&lt;Integer&gt; path = s.get(Student_.age); Predicate condition = cb.gt(path, age); // condition: attribute &gt; age cq.where(condition); // where TypedQuery&lt;Student&gt; tq = em.createQuery(cq); return tq.getResultList(); &#125;&#125; 执行findStudentByAgeLessThan方法如上图所示，报了一个NullPointerException，定位到了StudentRepositoryImpl类的这一行1Path&lt;Integer&gt; path = s.get(Student_.age); 尝试解决Debug重新运行之后，发现Student_.age变量为空，说明StaticMetaModelStudent_类并没有映射到Student类。之后我google了很多方法去解决这个问题，都没有用。但是可以确定的是一定是StaticMetaModel出了问题，因为将上面出错那一行的代码改为&quot;age&quot;之后，即：1Path&lt;Integer&gt; path = s.get(“age”); Bug解决了，但这似乎违背了Criteria查询的宗旨，就是类型安全和避免运行时错误，这里的安全类型除了指明确查询中的类型安全之外，还有就是避免使用常量。如果”age”字符串手抖写成了“aeg”，在编译时肯定是没有问题的，只有在运行时才会暴露出来，到了那个时候为时已晚。所以为了尽可能的遵循Criteria的初衷，这种方式肯定不是很好的方案，之后笔者又尝试了很多种方案，都宣告失败。于是我删掉了Student_类重新写了一次，但是这次Student和Student_位于同一包中，神奇的事情发生了，这次运行正确，并且成功查找出了年龄大于20的Student。 此时的工程结构 结果如下：123456789101112[ &#123; "id": "8a4ffa05622dcf1501622dd0aaa30003", "name": "bob", "age": 90 &#125;, &#123; "id": "8a4ffa05622dcf1501622dd0ca410004", "name": "an", "age": 30 &#125;] 结论Student和Student_必须位于同一包中。 更好的解决办法解决之后，本人始终百思不得其解，google了很多文章也没有解决。在之后的各种尝试中，发现了自动生成StaticMetaModelStudent_的方法，就是引入下面的依赖。12345&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-jpamodelgen&lt;/artifactId&gt; &lt;version&gt;1.1.1.Final&lt;/version&gt;&lt;/dependency&gt; 此依赖可以自动创建metamodel。现在我们删除我们自行创建的12```JavaPath&lt;Integer&gt; path = s.get(Student_.age); 此时程序会出现编译错误：因为刚才删除了Student_类，肯定会出现编译问题。抛开编译问题，这里我们继续执行，如下图所示，执行成功了。那么为什么呢？总结刚才的操作，你可能会猜测是不是刚才引入的依赖自动创建了Student_呢？答案是肯定的。为了证实这一点，我们去看编译运行的结果： 自动生成的Student_代码：12345678910111213package com.kylin.jpademo.domain;import javax.persistence.metamodel.SingularAttribute;import javax.persistence.metamodel.StaticMetamodel;@StaticMetamodel(Student.class)public abstract class Student_ &#123; public static volatile SingularAttribute&lt;Student, String&gt; name; public static volatile SingularAttribute&lt;Student, String&gt; id; public static volatile SingularAttribute&lt;Student, Integer&gt; age;&#125; 执行查询，没有问题。可以看出自动生成的Student_和Student在同一包下，这也印证了刚才的做法。 结论在Criteria查询中，Model和StaticMetaModel必须位于同一包下。因为在大型项目中，会涉及到很多Model，若不想自己创建对应的StaticMetaModel，可以使用hibernate-jpamodelgen依赖，自动创建。]]></content>
      <categories>
        <category>JPA</category>
      </categories>
      <tags>
        <tag>JPA</tag>
        <tag>Java</tag>
        <tag>Criteria</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈ADT及其应用]]></title>
    <url>%2F2018%2F03%2F14%2F%E6%A0%88ADT%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8_new%2F</url>
    <content type="text"><![CDATA[栈模型栈（Stack）是限制插入和删除只能在一个位子上进行的表，该位子是表的末端，叫做栈的顶（top）。对栈的基本操作有进栈（push）和出栈（pop），相当于表插入和删除最后一个元素。 栈的实现由于栈可以看作是一个表，因此任何实现表的方法都能实现栈。显然，在Java集合中，ArrayList和LinkedList都支持栈的操作，由于栈的操作都是栈顶元素，所以对于数组实现的ArrayList和链表实现的LinkedList来说都花费常数时间，因此他们并没有太大区别。但是对于代码实现来说，数组实现更加简洁，更易于理解，下面将给出栈的简单实现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Stack&lt;E&gt; &#123; //栈的默认大小为10 private static final int DEFAULT_VALUE_SIZE = 10; //用一个数组来存储栈的元素 private E[] elementData; //栈中元素的实际个数 private int size; //构造器，调用doClear方法初始化 public Stack() &#123; doClear(); &#125; //进栈操作 public E push(E element) &#123; //存放栈元素的数组满了，则需要对elementData扩容 if (elementData.length == size) &#123; //扩容操作 ensureCapacity(size + 1); &#125; elementData[size++] = element; return element; &#125; //出栈操作，若栈为空时出栈，则抛出数组越界异常 public E pop() &#123; if (size == 0) &#123; throw new IndexOutOfBoundsException(); &#125; return elementData[--size]; &#125; //清空栈的操作 public void clear() &#123; doClear(); &#125; //清空栈的操作 private void doClear() &#123; size = 0; ensureCapacity(DEFAULT_VALUE_SIZE); &#125; //保证在进栈的，存储栈元素的elementData数组有足够的空间 private void ensureCapacity(int minCapacity) &#123; if (size &gt; minCapacity) &#123; return; &#125; E[] old = elementData; elementData = (E[]) new Object[minCapacity]; for (int i = 0; i &lt; size; i++) &#123; elementData[i] = old[i]; &#125; &#125; //返回栈的实际元素个数 public int size() &#123; return size; &#125;&#125; 上面的代码实现了栈的基本操作，push/pop/clear。 栈的应用应用1 —— 平衡符号编译器检查程序的语法错误，但是常常由于缺少一个符号（如遗漏一个花括号）引起编译不能通过。在这种情况下，一个有用的工具就是检测是成对出现的东西。于是，一个右花括号、右方括号及右括号必然对其相应的左括号。比如[()]序列是合法的，[(])是不合法的。 通过栈，可以实现上述程序，思路如下：声明一个空栈，然后挨个读取字符，如果字符是一个开放符号（左边的符号），则将其推入栈内。如果字符是一个封闭符号（右边的符号），然后弹出栈顶元素，栈为空时报错。如果弹出的开放符号不是当前封闭符号所对应的符号，则报错。依次遍历，直到遍历完所有开放符号。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class StackApplication &#123; // 申明一个栈，用来存放开放符号 Stack&lt;Character&gt; lefts = new Stack&lt;Character&gt;(); // 申明一个数组，用来存放封闭符号 ArrayList&lt;Character&gt; rights = new ArrayList&lt;Character&gt;(); // 用来存放从控制台读取的字符数组 char[] characters; // 从控制台读取字符串，并转换成字符串 private void readCharsFromConsole() &#123; Scanner scanner = new Scanner(System.in); if (scanner.hasNext()) &#123; // 给characters数组 characters = scanner.next().toCharArray(); &#125; &#125; // 检查开放符号和封闭符号是否匹配 public boolean check() &#123; readCharsFromConsole(); // 开放符号进栈lefts，封闭符号添加到数组rights for (int i = 0; i &lt; characters.length; i++) &#123; char ch = characters[i]; if (ch == '(') &#123; lefts.push(ch); &#125; if (ch == ')') &#123; rights.add(ch); &#125; &#125; // 如果开放符号的个数不等于封闭符号的个数则报错 if (lefts.size() != rights.size()) &#123; return false; &#125; // 遍历封闭符号，如果栈lefts弹出的元素不是当前封闭元素所对应的，则返回false for (int i = 0; i &lt; rights.size(); i++) &#123; if (rights.get(i).charValue() == ')') &#123; if (lefts.pop().charValue() != '(') &#123; return false; &#125; &#125; &#125; // 最后返回true return true; &#125;&#125; 测试代码1234public static void main(String[] args) &#123; StackApplication stackApplication = new StackApplication(); System.out.println(stackApplication.check());&#125; 测试结果在控制台输入((()))，回车，结果如下图所示，返回true。 在控制台输入((())，回车，结果如下图所示，返回false。 应用二 —— 计算后缀表达式（逆波兰表达式)假设我们需要一个计算器来计算我们购物的花费，你的计算公式可能是这样的：2 * 2 + 3 * 3，如果你手上的计算器的普通计算器的话，答案为21。但是现在大多数时候我们希望使用科学计算器，它可以判定运算的优先级，科学计算器的结果为13。 我们简单地分析一下科学计算器计算上面例子的过程，首先判断优先级，乘法有限，所以依次计算2*2和#3*3并将其结果分别存储到两个临时变量A1和A2中，最后计算加法，将A1和A2求和。我们可以将这种操作写为：2 2 * 3 3 * +。这就是逆波兰表达式。 如何将2 * 2 + 3 * 3转化为2 2 * 3 3 * +，参见应用三。 计算细节如下： 首先假设有一个空栈stack，遍历后缀表达式，将数字压入栈中，直到遇到一个操作符，这个操作符的操作数为连续两次出栈的结果。 第一步：stack=[2,2]，读到一个操作符为*，于是操作数连续两次出栈的结果2和2。所以计算结果为2*2=4，将结果压入栈中，stack=[4]。 第二步：继续读入数字，直到一个操作符。stack=[4,3,3]，操作符为*，操作数为连续两次出栈的结果3和3。所以计算结果为3*3=9，将结果压入栈中，stack=[4,9]。 第三部：继续读入，下一个元素为+操作数，所以执行加法操作，操作数为连续两次出栈的结果4和9，所以计算结果为4+9=13。压栈，stack=13。到此遍历完了整个后缀表达式，最后结果就为栈顶元素13。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class StackApplication2 &#123; // 字符数组，用来保存后缀表达式的每一个字符 private char[] postfix; // 操作数栈 Stack&lt;Integer&gt; numberStack = new Stack&lt;Integer&gt;(); // 构造器，传入一个后缀表达式字符串 public StackApplication2(String postfix) &#123; this.postfix = postfix.toCharArray(); &#125; public StackApplication2() &#123; &#125; // 判断后缀表达式中字符是否为数字 private boolean isNumeric(char ch) &#123; return (ch &gt;= '0' &amp;&amp; ch &lt;= '9') ? true : false; &#125; // 科学计算器的实现方法 public int scientificCaculator() &#123; // 两个操作数 int num1; int num2; // 遍历后缀表达式 for (int i = 0; i &lt; postfix.length; i++) &#123; char temp = postfix[i]; // 如果是数字就压栈 if (isNumeric(temp)) &#123; numberStack.push(temp - '0');// char转int &#125; else &#123; // 如果是操作符就从栈中弹出操作数并执行相关运算 num1 = numberStack.pop(); num2 = numberStack.pop(); if (temp == '+') &#123; numberStack.push(num1 + num2); &#125; else if (temp == '-') &#123; numberStack.push(num1 - num2); &#125; else if (temp == '*') &#123; numberStack.push(num1 * num2); &#125; else if (temp == '/') &#123; numberStack.push(num1 / num2); &#125; &#125; &#125; // 返回最后的栈顶元素，即结果 return numberStack.pop(); &#125;&#125; 测试代码1234public static void main(String[] args) &#123; StackApplication2 stackApplication2 = new StackApplication2(&quot;6523+8*+3+*&quot;); System.out.println(stackApplication2.scientificCaculator());&#125; 结果计算一个后缀表示花费的时间是O(N)，该算法是一个十分简单的算法。注意，当一个表达式以后缀表示给出时，我们就不用知道运算的优先级，这是一个明显的优点。 应用三- 中缀表达式转后缀表达式栈不仅仅可以用来计算后缀表达式的值，还可以用来用来讲一个标准形式的表达式（中缀表达式）转换成后缀表达式。 我们假设只有运算+,*,(,)，并且表达式合法以及使用普通的优先级法则，即括号&gt;乘&gt;加。假设中缀表示1+2*3+(4*5+6)*7,则转换后的后缀表达式为：123*+45*6+7*+。 转换方法我们需要两个数据结构，一个用来存放操作符的栈operatorStack，一个用来存放后缀表达式的字符数组postfix。遍历中缀表达式，如果读到的是一个操作数，则立即添加到postfix数组中，如果是一个操作符则相对麻烦。为了说明方便，我们将operatorStack栈中的栈顶操作符称为top，当前遍历的操作符为temp。操作符的处理规则如下： 如果栈为空，则直接将temp压入operatorStack栈中。 如果栈不为空并且temp= * 或 + 或 (，然后将temp的优先级和top比较，如果temp的优先级大于top优先级，则将temp压栈；否则，依次将栈中优先级大于或等于temp的操作符弹出，添加到postfix，直到top优先级高于temp，然后将temp压栈，但是有一个情况例外，即top=（,temp的优先级低于(时，这种情况，top不会出栈，而是将temp直接压栈。 如果读到的），则依次弹出操作符，加入postfix的末尾，直到（。 现在我们对刚才提到的例子1+2*3+(4*5+6)*7采用上面的算法依次计算： 读取到操作数1，直接添加到postfix数组中。此时，operatorStack为空，postfix=[1]。 读取到操作符+，因为此时栈为空，所以直接压栈。此时，operatorStack={+}，postfix=[1]。 读取到操作数2，直接添加到postfix数组中。此时，operatorStack={+}，postfix=[1,2]。 读取到操作符*，此时temp = *，top = +，优先级temp &gt; top，所以temp压栈。此时，operatorStack={+,*}，postfix=[1,2]。 读取到操作数3，直接添加到postfix数组中。此时，operatorStack={+,*}，postfix=[1,2,3]。 读取到操作符+，此时temp = +,top = *，优先级top&gt;=temp，所以top出栈并添加到postfix数组。此时operatorStack={+}，postfix=[1,2,3,*]，top=+，top&gt;=temp，所以top出栈并添加到postfix数组。此时operatorStack={}，postfix=[1,2,3,*,+}。此时operatorStack为空，所以temp压栈。所以此时operatorStack={+}，postfix=[1,2,3,*,+}。 读取操作符（，此时temp = (，top = +，优先级temp &gt; top，所以temp直接压栈。此时，operatorStack={+,(}，postfix=[1,2,3,*,+]。 读取操作数4，直接添加到postfix数组中。此时operatorStack={+,(}，postfix=[1,2,3*,+，4]。 读取操作符*，此时temp = *，top = (，虽然优先级top &gt;= temp，但是对于（特殊处理，不出栈。所以temp直接压栈。此时operatorStack={+,(,*}，postfix=[1,2,3,*,+,4]。 读取操作数5，直接添加到postfix数组中。此时operatorStack={+,(,*}，postfix=[1,2,3,*,+,4,5]。 读取操作符+，此时temp = +，top = *，优先级top &gt;= temp，所以top出栈并且添加到postfix数组，此时operatorStack={+,(}，postfix=[1,2,3,*,+,4,5,*],top = （，优先级temp &lt; top，temp直接压栈。此时operatorStack={+,(,+}，postfix=[1,2,3,*,+,4,5,*]。 读取操作数6，直接添加到postfix数组中。此时operatorStack={+,(,+}，postfix=[1,2,3,*,+,4,5,*,6]。 读取操作符），依次弹出栈顶操作符，直到（。此时operatorStack={+}，postfix=[1,2,3,*,+,4,5,*,6,+]。 读取操作符*，此时temp = *，top = +，优先级temp &gt; top，所以temp压栈。此时operatorStack={+,*}，postfix=[1,2,3,*,+,4,5,*,6,+]。 读取操作数7，直接添加到postfix数组中。此时operatorStack={+,*}，postfix=[1,2,3,*,+,4,5,*,6,+,7]。 依次弹出栈中剩余操作符，最终operatorStack={}，postfix=[1,2,3,*,+,4,5,*,6,+,7,*,+]。 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class InfixToPostfix &#123; // 存放操作符的栈 private Stack&lt;Character&gt; operatorStack = new Stack&lt;Character&gt;(); // 存放后缀表达式的字符数组 private char[] postfix; // 存放中缀表达式的字符数组 private char[] infix; // 定义操作符 private static final char multiply = '*'; private static final char divide = '/'; private static final char add = '+'; private static final char substract = '-'; private static final char leftParenthesis = '('; private static final char rightParenthesis = ')'; // 构造器 public InfixToPostfix(String infix) &#123; this.infix = infix.toCharArray(); postfix = new char[infix.length()]; &#125; public InfixToPostfix() &#123; &#125; // 判断一个字符是否为数字 private boolean isNumeric(char ch) &#123; return (ch &gt;= '0' &amp;&amp; ch &lt;= '9') ? true : false; &#125; private String infixToPostfix() &#123; // 插入后缀的表达式下标 int insertIndex = 0; // 操作符栈的大小 int size; // 返回结果，后缀表达式字符串 String result; for (int i = 0; i &lt; infix.length; i++) &#123; if (isNumeric(infix[i])) &#123; postfix[insertIndex++] = infix[i]; &#125; else &#123; char temp = infix[i]; size = operatorStack.size(); // 如果栈为空，就直接压栈 if (size == 0) &#123; operatorStack.push(temp); &#125; else &#123; // 栈不为空时 // 弹出栈顶元素 char top = operatorStack.pop(); if (temp == rightParenthesis) &#123; while (top != leftParenthesis) &#123; postfix[insertIndex++] = top; top = operatorStack.pop(); &#125; &#125; else &#123; // 如果当前操作符的优先级大于栈顶操作符的优先级，则当前操作符进栈 if (operatorPriorityCompare(temp, top)) &#123; /* * 因为上一行为了比较当前操作符和栈顶元素的操作符, * 上面弹出了栈顶元素, * 所以这里要先将弹出的操作符压栈 */ operatorStack.push(top); &#125; /* * 如果当前操作符的优先级低于或等于栈顶操作符的优先级， 则栈顶操作符出栈，然后一直比较， * 直到栈顶操作符优先级大于当前操作符或栈为空，然后当前操作符进栈。 */ while (!operatorPriorityCompare(temp, top)) &#123; if (top == leftParenthesis) &#123; operatorStack.push(top); break; &#125; else &#123; postfix[insertIndex++] = top; size = operatorStack.size(); if (size == 0) &#123; break; &#125; top = operatorStack.pop(); &#125; &#125; // 当前操作符进栈 operatorStack.push(temp); &#125; &#125; &#125; &#125; // 遍历完毕，一次弹出栈中剩余操作符 size = operatorStack.size(); if (size != 0) &#123; for (int i = 0; i &lt; size; i++) &#123; postfix[insertIndex++] = operatorStack.pop(); &#125; &#125; result = String.valueOf(postfix).trim(); return result; &#125; // 判断操作符的优先级：括号&gt;乘除&gt;加减 // true: temp&gt;top false: top &gt;= temp private boolean operatorPriorityCompare(char current, char top) &#123; if (((current == multiply || current == divide) &amp;&amp; (top == substract || top == add)) || (current == leftParenthesis &amp;&amp; (top == multiply || top == divide || top == substract || top == add))) &#123; return true; &#125; else &#123; return false; &#125; &#125;&#125; 测试代码12345public static void main(String[] args) &#123; InfixToPostfix infixToPostfix = new InfixToPostfix("1+2*3+(4*5+6)*7"); String result = infixToPostfix.infixToPostfix(); System.out.println(result);&#125; 测试结果与上面相同，这种转换需要O(N)时间。可以通过制定减法和加法有相同优先级以及乘法和除法有相同优先级二将减法和乘法添加到指令中去。需要注意的是，表达式a-b-c应该转化为ab-c-，而不是abc–。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树(binary tree)]]></title>
    <url>%2F2018%2F03%2F14%2F%E4%BA%8C%E5%8F%89%E6%A0%91-binary-tree_new%2F</url>
    <content type="text"><![CDATA[二叉树的定义二叉树是n(n&gt;=0)个具有相同类型的元素的有限集合，当n=0时称为空二叉树，当n&gt;0时，数据元素被分为一个称为根（Root）的数据元素及两棵分别为左子树和右子树的数据元素的集合，左、右子树互不相交，且左、右子树都为二叉树。在二叉树中，一个元素也称为一个节点。 二叉树的子树是有序的，即若将其左、右子树颠倒，就将成为另一棵不同的二叉树。即使树中的节点只有一棵子树，也要明确指出其是左子树还是右子树。由于左、右子树的有序以及二叉树可以为空，因此，二叉树具有以下五种基本形态，即空二叉树、仅有根节点的二叉树、右子树为空的二叉树、左子树为空的二叉树、左右子树均为非空的二叉树，如下图所示： 二叉树的基本概念 节点的度：节点所拥有子树的个数称为该节点的度。 叶子：度为0的节点称为叶子。 孩子：节点子树的根，称为该节点的孩子。二叉树中，孩子有左右之分，分别为左孩子和右孩子。 双亲：孩子节点的上层节点都称为该节点的双亲。 以某节点的根的子树中的除根节点之外的任意一个节点都称为该节点的子孙。 祖先：节点的祖先是从根到该节点所经分支上的所有节点。 节点的层次：从根节点起，跟为第一层，他的孩子为第二层，孩子的孩子为第三层，依次类推，即某个节点的层次为L层，那么他的孩子节点的层数为L+1层。 兄弟：同一双亲的孩子互为兄弟。 堂兄弟：其双亲在同一层的节点互为堂兄弟。 二叉树的度：二叉树中最大的节点度称为二叉树的度。 二叉树的深度：二叉树中节点的最大层次书被称为二叉树的深度。 满二叉树：在一棵二叉树中，所有分支节点都存在左子树和右子树，并且所有叶子节点都在同一层上，这样的二叉树被称为满二叉树。 完全二叉树：对深度为k的满二叉树中的节点从上至下、从左至右从1开始编号。对一棵具有n个节点、深度为k的二叉树，采用的同样的办法对树中的节点从上至下，从左至右从1开始连续编号，如果编号为i（i&lt;=n）的节点与满二叉树中编号为i的节点在同一位置，则称此二叉树为完全二叉树。对于一棵完全二叉树，其叶子节点只可能出现在最下层和倒数第二层，而最下层的叶子集中在树的最左部。 二叉树的性质 一棵非空二叉树的第i层最多有2^(i-1)个节点。 深度为k的二叉树至多有2^k-1个节点。 对任何一棵二叉树T，如果叶子节点数为n0，度为2的节点数为n2，那么n0=n2+1。 具有n个节点的安全二叉树的深度k=log2N+1（这里解释一下，k=以2为底N的对数向下取整+1）。 对一棵具有n个节点的完全二叉树的从上至下，从左至右从1开始连续编号，那么对任意一个节点i有：如果i=1,则节点i是二叉树的根，无双亲；如果i&gt;1，则其双亲是i/2向下取整。如果2i&gt;n，则节点无左孩子，为叶子节点；如果2i&lt;n，则其左孩子是2i。如果2i+1&gt;n，则节点i无右孩子；如果2i+1&lt;=n，则其右孩子为2i+1。 二叉树的存储结构顺序存储完全二叉树的顺序存储我们先看以下完全二叉树的顺序存储。要存储一棵完全二叉树，不仅需要二叉树的节点数据，还需要存储它的结构信息，即双亲和孩子关系信息。从上面的性质5可以看出，完全二叉树中节点i的左孩子为2i，右孩子为2i+1，双亲为i/2向下取整。因此，如果将完全二叉树从上至下，从左至右从1开始编号，把编号为i的节点放在线性存储单元的第i个位子，那么其左孩子存储在2i位子，右孩子存储在2i+1位子，则孩子和双亲的关系就体现出来了。一般二叉树的顺序存储对于一般二叉树而言，如果把它的节点按照从上至下、从左至右从1开始连续编号存储在一维的存储单元，那么无法反应其节点间的双亲孩子关系，即不能反映二叉树的结构关系，怎么办呢?可以把一般的二叉树补成一棵完全二叉树，这样，它就可以按照完全二叉树的顺序存储方式存储了，只是新补上去的节点只占位子，不存放节点数据。如下图所示： 对于一般二叉树，需要增加一些节点才能存储此二叉树的节点数据和结构关系，这样可能会造成存储空间的浪费，例如，对于深度为3的右偏斜二叉树，需要额外增加4个存储单位。当二叉树的深度更深时，则需要更多的节点，例如当深度为100的右偏斜二叉树，需要2^100-101个额外空间，为了采用顺序存储方式来存储此二叉树，把全世界所有计算机的存储空间加起来也不够！因此很有必要采用其他形式的存储方式。 链式存储链表可以用来表示一维的线性结构，也可以用来表示非线性的二叉树结构。二叉树的链式存储结构常有二叉链表存储、三叉链表存储及线索链表。二叉链表中有两个指针域，分别指向其左、右孩子；三叉链表中除了指向其左、右孩子的指针域外，还有指向其双亲的指针域；线索链表是为了反映节点的前驱、后继而将二叉链表中空指针指向其前驱或后继而形成的链式存储结构。 二叉链表存储链表中每一个节点包含3个域：数据域、左孩子指针域、右孩子指针域。左、右孩子指针域分别指向左孩子和右孩子的存储地址。 二叉链表存储代码实现12345678910111213141516171819202122232425262728293031323334353637383940public class BinaryTreeNode&lt;T&gt; &#123; private T data; private BinaryTreeNode&lt;T&gt; leftChild; private BinaryTreeNode&lt;T&gt; rightChild; public BinaryTreeNode(T data) &#123; this(data, null, null); &#125; public BinaryTreeNode(T data, BinaryTreeNode&lt;T&gt; leftChild, BinaryTreeNode&lt;T&gt; rightChild) &#123; this.leftChild = leftChild; this.rightChild = rightChild; this.data = data; &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125; public BinaryTreeNode&lt;T&gt; getLeftChild() &#123; return leftChild; &#125; public void setLeftChild(BinaryTreeNode&lt;T&gt; leftChild) &#123; this.leftChild = leftChild; &#125; public BinaryTreeNode&lt;T&gt; getRightChild() &#123; return rightChild; &#125; public void setRightChild(BinaryTreeNode&lt;T&gt; rightChild) &#123; this.rightChild = rightChild; &#125;&#125; 三叉链表存储在三叉链表中，除根节点的parent域为空外，其余节点的parent域都不为空，指向其双亲。因此在三叉链表中，查找孩子和双亲都是很快捷的，但是增加了一些额外空间开销。 三叉链表存储实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class BinaryTreeNode3&lt;T&gt; &#123; private T data; private BinaryTreeNode3&lt;T&gt; leftChild; private BinaryTreeNode3&lt;T&gt; parent; private BinaryTreeNode3&lt;T&gt; rightChild; // 叶子节点构造器 public BinaryTreeNode3(T data, BinaryTreeNode3&lt;T&gt; parent) &#123; this(data, null, parent, null); &#125; // 一般节点构造器 public BinaryTreeNode3(T data, BinaryTreeNode3&lt;T&gt; leftChild, BinaryTreeNode3&lt;T&gt; parent, BinaryTreeNode3&lt;T&gt; rightChild) &#123; this.leftChild = leftChild; this.parent = parent; this.rightChild = rightChild; this.data = data; &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125; public BinaryTreeNode3&lt;T&gt; getLeftChild() &#123; return leftChild; &#125; public void setLeftChild(BinaryTreeNode3&lt;T&gt; leftChild) &#123; this.leftChild = leftChild; &#125; public BinaryTreeNode3&lt;T&gt; getParent() &#123; return parent; &#125; public void setParent(BinaryTreeNode3&lt;T&gt; parent) &#123; this.parent = parent; &#125; public BinaryTreeNode3&lt;T&gt; getRightChild() &#123; return rightChild; &#125; public void setRightChild(BinaryTreeNode3&lt;T&gt; rightChild) &#123; this.rightChild = rightChild; &#125;&#125; 二叉树的遍历先序、中序、后续的递归遍历例子 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class BinaryTree&lt;T&gt; &#123; public BinaryTree() &#123; // ... &#125; // 访问数据 public void visitData(BinaryTreeNode&lt;T&gt; node) &#123; System.out.print(node.getData() + " "); &#125; // 前序遍历 public void preOrder(BinaryTreeNode&lt;T&gt; node) &#123; if (null != node) &#123; visitData(node); preOrder(node.getLeftChild()); preOrder(node.getRightChild()); &#125; &#125; // 中序遍历 public void inOrder(BinaryTreeNode&lt;T&gt; node) &#123; if (null != node) &#123; inOrder(node.getLeftChild()); visitData(node); inOrder(node.getRightChild()); &#125; &#125; // 后续遍历 public void postOrder(BinaryTreeNode&lt;T&gt; node) &#123; if (null != node) &#123; postOrder(node.getLeftChild()); postOrder(node.getRightChild()); visitData(node); &#125; &#125; public static void main(String[] args) &#123; BinaryTreeNode&lt;String&gt; g = new BinaryTreeNode&lt;String&gt;("g"); BinaryTreeNode&lt;String&gt; c = new BinaryTreeNode&lt;String&gt;("c"); BinaryTreeNode&lt;String&gt; d = new BinaryTreeNode&lt;String&gt;("d"); BinaryTreeNode&lt;String&gt; b = new BinaryTreeNode&lt;String&gt;("b", c, d); BinaryTreeNode&lt;String&gt; f = new BinaryTreeNode&lt;String&gt;("f", g, null); BinaryTreeNode&lt;String&gt; e = new BinaryTreeNode&lt;String&gt;("e", null, f); BinaryTreeNode&lt;String&gt; a = new BinaryTreeNode&lt;String&gt;("a", b, e); BinaryTree&lt;String&gt; binaryTree = new BinaryTree&lt;String&gt;(); System.out.print("preOder:"); binaryTree.preOrder(a); System.out.println(); System.out.print("inOder:"); binaryTree.inOrder(a); System.out.println(); System.out.print("postOder:"); binaryTree.postOrder(a); System.out.println(); &#125;&#125; 执行结果 非递归遍历的思想当非递归遍历时，我们需要从根节点开始，从左到右深入到每一个叶子。当深入到一个叶子节点时，需要返回到其父节点，然后去深入其他分支。可以看出深入和返回是一对相反的操作，所以可以用到数据结构 栈来保存深入节点时树的结构关系。至于该遍历是先序、中序或是后序，这只取决于其访问 非递归的中序遍历沿左子树深入时，深入一个节点入栈一个节点，沿左分支无法继续深入时（某个节点无左孩子），出栈，出站时同时访问节点数据，然后从该节点的右子树继续沿左子树深入，这样一直下去，最后从根节点的右子树返回时结束。123456789101112131415161718192021222324// 非递归的中序遍历public void nrInOrder(BinaryTreeNode&lt;T&gt; root) &#123; // 声明一个栈 Stack&lt;BinaryTreeNode&lt;T&gt;&gt; stack = new Stack&lt;BinaryTreeNode&lt;T&gt;&gt;(); // 当前节点 BinaryTreeNode&lt;T&gt; p = root; // 当节点和栈不同时为空时 while (!(p == null &amp;&amp; stack.isEmpty())) &#123; // 遍历p节点下的所有左孩子 while (p != null) &#123; // 将左孩子压栈 stack.push(p); p = p.getLeftChild(); &#125; if (stack.isEmpty()) &#123; return; &#125; else &#123; p = stack.pop();// 出栈 visitData(p);// 出栈时访问数据 p = p.getRightChild();// 指向右孩子 &#125; &#125;&#125; 非递归的层次遍历从根节点开始，根节点入队，访问其数据，然后根节点的左右孩子入队，根节点出队。此时相当于第一层遍历完毕。第二层数据已经入队。然后当前队首元素出队，访问数据，加入当前元素的左右孩子，依次类推直到队列为空。123456789101112131415161718192021222324252627public void levelOrder(BinaryTreeNode&lt;T&gt; root) &#123; // 声明一个队列 Queue&lt;BinaryTreeNode&lt;T&gt;&gt; queue = new LinkedList&lt;BinaryTreeNode&lt;T&gt;&gt;(); // 如果二叉树为空，直接返回 if (null == root) &#123; return; &#125; // 根节点入队 queue.add(root); // 临时变量，用来保存上一次出队的元素 BinaryTreeNode&lt;T&gt; temp; // 遍历队列，直到队列为空 while (!queue.isEmpty()) &#123; // 出队 temp = queue.remove(); // 访问刚才出队元素的数据 visitData(temp); // 若刚才出队的元素（节点）有左孩子，则左孩子入队 if (null != temp.getLeftChild()) &#123; queue.add(temp.getLeftChild()); &#125; // 若刚才出队的元素（节点）有左孩子，则左孩子入队 if (null != temp.getRightChild()) &#123; queue.add(temp.getRightChild()); &#125; &#125; &#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>二叉树</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker搭建Redis主从复制、哨兵机制]]></title>
    <url>%2F2018%2F03%2F14%2F%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BARedis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E3%80%81%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6_new%2F</url>
    <content type="text"><![CDATA[拓扑结构本文搭建如下图所示的redis拓扑结构，拓扑中共有3个哨兵，1和mater结点和2个slave结点。拓扑信息： 角色 ip port redis-master redis-master或者master或者master-sentinel或者master-sentinel2或者master-sentinel3或自动分配的ip 6379 redis-slave1 容器启动时自动分配的ip 6380 redis-slave2 容器启动时自动分配的ip 6381 redis-master-sentinel 容器启动时自动分配的ip 26379 redis-master-sentinel2 容器启动时自动分配的ip 26380 redis-master-sentinel3 容器启动时自动分配的ip 26381 注：master或者master-sentinel或者master-sentinel2或者master-sentinel3都是主节点容器的别名。与之相关联的容器可以通过别称来代替ip访问容器。 搭建主从复制结构master节点配置master节点配置文件 master节点配置文件详解 port：端口号 daemonize：redis采用的是单进程多线程的模式。当redis.conf中选项daemonize设置成yes时，代表开启守护进程模式。在该模式下，redis会在后台运行，并将进程pid号写入至redis.conf选项pidfile设置的文件中，此时redis将一直运行，除非手动kill该进程。 logfile：日志文件的名字，在工作目录下自动创建，会记录redis的运行日志。 dbfilename：dump文件的名字，在工作目录下自动创建。 dir：工作目录的路径。 其他配置请参考redis中文官网：www.redis.cn 在docker中运行redis master节点服务命令说明： --name: 命名容器为redis-master，一定不要忽略容器命名，为后面--link命令命名更容易记住名字。 -v:将本地的配置文件~/redis/redis-6379.conf挂载到容器中的/redis/redis-6379.conf 容器启动后，进入容器挂载的目录，执行redis-server redis-6379.conf命令，使用redis-6379.conf配置启动redis服务。具体操作如下图所示：执行上述命令之后，如果没有消息，那么就是最好的消息，说明redis-server已经成功启动了。可以通过redis-6379.conf中配置的日志文件来查看启动情况。具体操作如下图所示：日志说明： 第一行：redis正在启动 第二行：redis版本信息，64位，进程号等 第三行：加载配置 第四行：redis的运行模式为standalone，端口为6379 第五行：警告，这句话的翻译大概就是：对一个高负载的环境来说tcp设置128这个值，太小了。具体解决方案参考：https://www.cnblogs.com/faunjoe88/p/7158484.html 第六行：说明redis server已经成功初始化 第七行： 警告，THP的系统配置问题，具体参考：https://jingyan.baidu.com/article/da1091fb196ea7027849d6b0.html上述警告对于demo来说可以忽略，但是对于生产环境，需要重新对系统进行相关配置之后，再重新启动容器。 至此，master节点已经成功启动了。 slave节点配置slave节点配置文件两个slave节点的配置一模一样，除了端口号之外。slave1的端口为6380，slave2的端口号为6381。 slave节点配置文件详解 前5项配置和master节点类似，请参考master节点配置说明。 这里重点说一下 slaveof命令，此命令用来给当前redis server节点指定一个master节点，自身作为master节点的slave节点。slaveof命令的格式为slaveof &lt;ip&gt; &lt;port&gt;，很明显，当前节点通过ip和port来定位将哪一个节点作为master节点，但是对于配置slaveof redis-master 6379来说，redis-master参数并不是一个ip。这里是因为在docker环境下，容器启动是ip是不定的，所以容器的通信可以通过--link选项来实现，而这里的redis-master就是master节点容器的名字，用容器名可以代替ip。具体参看下文。 在docker中运行redis slave节点服务命令说明： --name: 容器名为redis-slave1 -v: 将本地~/redis/redis-6380.conf挂载到容器目录/redis/redis-6380.conf --link: 建立与master节点之间的容器间的通信，redis-master为master节点的容器名，master为redis-master的别名。因此，slave的配置文件redis-6380.conf中最后一项配置也可以配置为slaveof master 6379。 容器启动后使用redis-server redis-6380.conf命令启动redis server服务。如下图所示：redis-server redis-6380.conf命令执行后，如果没有任何消息，那么就是最好的消息，说明redis server已经成功启动。下面可以通过查看日志文件来查看启动情况。如下图所示：日志分析： 从第二行可以看出redis-server已经启动成功了。 Connecting to MASTER redis-master:6379说明已经连接到master节点，并且开始了数据的同步，从master节点复制到slave节点。 从最后6行可以看出，因为是新增的slave结点，所以master到slave的复制时全量复制（倒数第五行），部分复制不可用（倒数第六行）。复制一共经历了四个步骤（最后四行）：1. 从master接收数据 2.清理掉旧的数据 3.在内存中接在db 4.复制成功。 配置redis哨兵redis哨兵配置文件配置说明： port daemonize logfile dir配置和普通redis server节点相同。 sentinel monitor：该命令的格式为sentinel monitor &lt;master&gt; &lt;ip&gt; &lt;port&gt; &lt;quorum&gt;。&lt;master&gt;：参数为哨兵监控的master节点的别名&lt;ip&gt;：参数为监控的master节点的ip（在docker中，容器间用--link命令通信，所以可以替换为目标容器的名字或别名）&lt;port&gt;：为监控的master结点的端口&lt;quorum&gt;：代表要判定master节点最终不可达所需要的票数。用于故障发现和判定。例如如果将quorum配置为2，代表至少要两个哨兵节点认为master节点不可达，那么这个不可达的判定才是客观的，对于值设置的越小，那么达到下线的条件就越宽松，反之越严格。一般建议将其设置为哨兵节点的数量加1。 sentinel down-after-milliseconds命令格式为sentinel down-after-milliseconds &lt;master-name&gt; &lt;times&gt;&lt;master&gt;：参数为主节点的名称，这里为上面设置的mymaster&lt;times&gt;：sentinel节点定期会想master节点发送ping命令，如果超过times毫秒没有收到回复，则判定该节点不可达。down-after-milliseconds虽然以为参数，但实际上对哨兵节点、主节点、从节点的判定同时有效，可以通过主节点来获取从节点和哨兵节点的信息。 sentinel parallel-syncs格式为sentinel parallel-syncs &lt;master-name&gt; &lt;nums&gt;当哨兵节点集合对主节点的故障判定达到一致时，哨兵领导节点会做故障转移操作，选出新的主节点，原来的从节点会向新的主节点发起复制操作，parallel-syncs参数就是限制从节点向新的主节点发起复制的个数。若发起复制的从节点过多，那么可能会造成主节点阻塞。若发起复制的从节点过少，可能会造成数据在复制期间不一致的情况。 sentinel failover-timeout 格式为sentinel failover-timeout &lt;master-name&gt; &lt;times&gt;表示故障转移的超时时间。其他配置请参加redis中文官网：www.redis.cn 在docker容器中启动redis哨兵服务使用下图命令在容器中启动redis哨兵服务。其他两个哨兵服务只需要修改：--name参数（比如 redis-master-sentinel2)-v挂载相应的配置文件(比如~/redis/redis-26380.conf:/redis/redis-62380.conf)，–link参数给主节点去不同的别名(比如redis-master:master-sentinel2)。在容器中启动redis-sentinel服务：运行上述命令后没有消息，就是最好的消息。下面可以查看工作目录下的日志文件来查看启动情况。如下图所示：日志说明： 从第四行可以看出，节点启动成功，以sentinel模式运行，端口为26379。 倒数第二行为sentinel的id信息 最后一行说明新加了一个哨兵节点监控到master节点，名字为mymaster，ip为172.17.0.2，quorum为2。 至此，redis-sentinel节点配置完毕，其余两个sentinel节点请读者根据上面配置自行配置完成（很容易）。 总结本文从配置角度描述了怎么使用Docker搭建redis主从复制，并且添加了哨兵机制，但是并没有对redis进行详细剖析，这里强烈建议读者阅读相关书籍或是到redis官网了解redis运行机制。若有问题，欢迎在评论区留言。本文会定期更新，以便使用跟新版本的redis和docker。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
</search>
